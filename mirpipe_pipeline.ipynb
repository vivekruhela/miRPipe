{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Open source RNA-Seq pipeline for identification of novel-mirs and their gene regulatory networks<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MiRPipe-Flowchart\" data-toc-modified-id=\"MiRPipe-Flowchart-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MiRPipe Flowchart</a></span></li><li><span><a href=\"#FastQ-Files\" data-toc-modified-id=\"FastQ-Files-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>FastQ Files</a></span></li><li><span><a href=\"#Pre-processing\" data-toc-modified-id=\"Pre-processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quality-Check-of-Fastq-Files\" data-toc-modified-id=\"Quality-Check-of-Fastq-Files-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Quality Check of Fastq Files</a></span></li><li><span><a href=\"#Adaptor-Timming-and-Fastq-Splitting\" data-toc-modified-id=\"Adaptor-Timming-and-Fastq-Splitting-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Adaptor Timming and Fastq Splitting</a></span></li></ul></li><li><span><a href=\"#Sequence-Alignment\" data-toc-modified-id=\"Sequence-Alignment-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sequence Alignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Downloading-the-Mirdeep*-aligner-and-mirbase\" data-toc-modified-id=\"Downloading-the-Mirdeep*-aligner-and-mirbase-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><strong>Downloading the Mirdeep* aligner and mirbase</strong></a></span></li><li><span><a href=\"#piRNA-Pipeline\" data-toc-modified-id=\"piRNA-Pipeline-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>piRNA Pipeline</a></span></li><li><span><a href=\"#miRNA-sequence-Alignment\" data-toc-modified-id=\"miRNA-sequence-Alignment-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>miRNA sequence Alignment</a></span></li></ul></li><li><span><a href=\"#Post-Alignment-Analysis\" data-toc-modified-id=\"Post-Alignment-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Post-Alignment Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Processing-of-raw-counts-from-Mirdeep*-results\" data-toc-modified-id=\"Processing-of-raw-counts-from-Mirdeep*-results-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><strong>Processing of raw counts from Mirdeep* results</strong></a></span></li></ul></li><li><span><a href=\"#Blast-Search\" data-toc-modified-id=\"Blast-Search-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Blast Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Search-for-Novel-miRNA-sequence-in-DashR-Database\" data-toc-modified-id=\"Search-for-Novel-miRNA-sequence-in-DashR-Database-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Search for Novel miRNA sequence in DashR Database</a></span></li><li><span><a href=\"#DashR-Results-post-processing\" data-toc-modified-id=\"DashR-Results-post-processing-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>DashR Results post-processing</a></span></li></ul></li><li><span><a href=\"#Seed-based-Sequence-Clustering\" data-toc-modified-id=\"Seed-based-Sequence-Clustering-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Seed based Sequence Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dictionary-Preparation\" data-toc-modified-id=\"Dictionary-Preparation-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span><strong>Dictionary Preparation</strong></a></span></li><li><span><a href=\"#CD-HIT\" data-toc-modified-id=\"CD-HIT-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>CD-HIT</a></span></li><li><span><a href=\"#Seed-Clustering-Algorithm\" data-toc-modified-id=\"Seed-Clustering-Algorithm-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Seed Clustering Algorithm</a></span></li></ul></li><li><span><a href=\"#miRNA-Differential-expression-Analysis-using-DESeq2\" data-toc-modified-id=\"miRNA-Differential-expression-Analysis-using-DESeq2-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>miRNA Differential expression Analysis using DESeq2</a></span></li><li><span><a href=\"#piRNA-Counts-Analysis\" data-toc-modified-id=\"piRNA-Counts-Analysis-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>piRNA Counts Analysis</a></span></li><li><span><a href=\"#piRNA-Differential-expression-Analysis-using-DESeq2\" data-toc-modified-id=\"piRNA-Differential-expression-Analysis-using-DESeq2-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>piRNA Differential expression Analysis using DESeq2</a></span></li><li><span><a href=\"#Finding-significantly-altered-differentially-expressed-miRNAs\" data-toc-modified-id=\"Finding-significantly-altered-differentially-expressed-miRNAs-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Finding significantly altered differentially expressed miRNAs</a></span></li><li><span><a href=\"#Output\" data-toc-modified-id=\"Output-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Output</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiRPipe Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](miRPipe_Flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastQ Files\n",
    "**Loading libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.057874Z",
     "start_time": "2019-05-20T12:51:43.698812Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import random\n",
    "from splinter import Browser\n",
    "import time\n",
    "from operator import add\n",
    "from os import path\n",
    "import threading\n",
    "from threading import Semaphore\n",
    "screenlock = Semaphore(value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.121790Z",
     "start_time": "2019-05-20T12:51:49.062252Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declaring ENV Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.151397Z",
     "start_time": "2019-05-20T12:51:49.135554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please be sure that all the fastq files are present in /mnt/disk2/MiRPipe_Docker/data. All the results will be saved in the same folder also.\n"
     ]
    }
   ],
   "source": [
    "# setting env variables:\n",
    "os.environ['HOME_DIR'] = os.getcwd()\n",
    "os.environ['SEQ_DIR'] = os.path.join(os.environ['HOME_DIR'],'data')\n",
    "os.environ['Tools_DIR'] = os.path.join(os.environ['HOME_DIR'],'Tools')\n",
    "if not os.path.isdir(os.path.join(os.environ['SEQ_DIR'],'LOG_DIR')):\n",
    "    os.mkdir(os.path.join(os.environ['SEQ_DIR'],'LOG_DIR'))\n",
    "    os.environ['LOG_DIR'] = os.path.join(os.environ['SEQ_DIR'],'LOG_DIR')\n",
    "else:\n",
    "    os.environ['LOG_DIR'] = os.path.join(os.environ['SEQ_DIR'],'LOG_DIR')\n",
    "    \n",
    "os.environ['REF_DIR'] = os.path.join(os.environ['HOME_DIR'], 'Tools',\n",
    "                                     'MDS_command_line_v38/MDS_command_line')\n",
    "print('Please be sure that all the fastq files are present in %s. All the results will be saved in the same folder also.' %os.environ['SEQ_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that path is correct and fastq files are present in this path\n",
    "For Smooth Functioning of Pipeline, please make sure that only input fastq files are present in the data folder.\n",
    "Delete all the clutter before re-running the pipeline for trouble-free execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.634658Z",
     "start_time": "2019-05-20T12:51:49.156447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/disk2/MiRPipe_Docker\n",
      "/mnt/disk2/MiRPipe_Docker/data\n",
      "/mnt/disk2/MiRPipe_Docker/data/LOG_DIR\n",
      "/mnt/disk2/MiRPipe_Docker/Tools/MDS_command_line_v38/MDS_command_line\n"
     ]
    }
   ],
   "source": [
    "!echo $HOME_DIR\n",
    "!echo $SEQ_DIR\n",
    "!echo $LOG_DIR\n",
    "!echo $REF_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "## Quality Check of Fastq Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Following script perform the following task:\n",
    "    1. Perform quality checking of rach samples using FastQC.\n",
    "    2. Prepare the better info-graphic and detailed report of all quality checking\n",
    "       reports from all samples using multiqc\n",
    "'''\n",
    "!Rscript $HOME_DIR/scripts/FastQC.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptor Timming and Fastq Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The default adaptor sequence used for adaptor trimming is \\\n",
    "TCGTATGCCGTCTTCTGCTTG. If your adaptor sequence is different then please \\\n",
    "enter your data specific adaptor sequence. Please selec appropriate option.')\n",
    "print('1. Default adaptor sequence: TCGTATGCCGTCTTCTGCTTG')\n",
    "print('2. User specific adaptor sequence')\n",
    "user_choice =int(input('Please select your option:'))\n",
    "\n",
    "if user_choice == 1:\n",
    "    adaptor = 'TGGAATTCTCGGGTGCCAAGG'\n",
    "    os.environ['adaptor'] = adaptor\n",
    "elif user_choice == 2:\n",
    "    adaptor = input('Please enter the adaptor sequence')\n",
    "    os.environ['adaptor'] = adaptor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add script for adaptor trimming here using trim_galore\n",
    "!bash scripts/adaptor_trimming.sh 1> data/LOG_DIR/adaptor_trimming.stdout 2>data/LOG_DIR/adaptor_trimming.stderr\n",
    "\n",
    "# Add script for read length based spliting (into 3 parts) here using bbduk\n",
    "!bash scripts/fastq_split.sh 1> data/LOG_DIR/fastq_split.stdout 2> data/LOG_DIR/fastq_split.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading the Mirdeep* aligner and mirbase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Please enter your choice: ')\n",
    "print(\"Option 1: hg19 based alignment using Mirdeep* with miRBase v19 \")\n",
    "print(\"Option 2: hg19 based alignment using Mirdeep* with miRBase v20\")\n",
    "print(\"Option 3: hg38 based alignment using Mirdeep* with miRBase 21\")\n",
    "print(\"Option 4: hg38 based alignment using Mirdeep* with miRBase 22 (Default Condition)\")\n",
    "print(\"Please enter either 1, 2, 3 or 4\")\n",
    "choice = int(input(\"Please enter your choice:  \"))\n",
    "\n",
    "if choice == 1:\n",
    "    print(\"You chose hg19 and miRBase v19 based alignment using Mirdeep*\")\n",
    "    conf_file = open(os.path.join(os.environ['Tools_DIR'],'mirdeep.conf'),\"w\")    \n",
    "    line = \"Human Genome = hg19\" + \"\\n\"\n",
    "    line += \"miRBase = 19\"\n",
    "    conf_file.write(line)    \n",
    "    print(\"Downloading Mirdeep*....\")\n",
    "    command = \"wget --content-disposition  http://sourceforge.net/projects/mirdeepstar/files/MDS_command_line_v37.zip -P $Tools_DIR && \"\n",
    "    command += \"unzip -o $Tools_DIR/MDS_command_line_v37.zip -d $Tools_DIR\"\n",
    "    subprocess.call(command, shell=True)    \n",
    "    print(\"Downloading Mirdeep* is complete. Now downloading hg19 human reference genome....\")\n",
    "    ref_var = 'hg19'    \n",
    "    os.environ['REF_DIR'] = os.path.join(os.environ['HOME_DIR'], 'Tools',\n",
    "                                     'MDS_command_line_v37/MDS_command_line')\n",
    "    print(\"\")\n",
    "    command = \"\"\n",
    "    command += \"mkdir $REF_DIR/hg19 && \"\n",
    "    command += \"wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz -P $REF_DIR/hg19 && \"\n",
    "    command += \"pigz -p 5 -d $REF_DIR/hg19/hg19.fa.gz && echo Download complete. Now building bowtie indexes && \"\n",
    "    command += \"$HOME_DIR/Tools/bowtie-1.2.3-linux-x86_64/bowtie-build --threads 8 $HOME_DIR/refs/hg19/hg19.fa $HOME_DIR/refs/hg19/hg19\"\n",
    "    subprocess.call(command, shell=True)  \n",
    "    print(\"Human refenrence genome hg19 has been downloaded and index files generation complete.\")\n",
    "\n",
    "elif choice == 2:\n",
    "    print(\"You chose hg19 and miRBase v20 based alignment using Mirdeep*\")\n",
    "    conf_file = open(os.path.join(os.environ['Tools_DIR'],'mirdeep.conf'),\"w\")    \n",
    "    line = \"Human Genome = hg19\" + \"\\n\"\n",
    "    line += \"miRBase = 20\"\n",
    "    conf_file.write(line)    \n",
    "    command = \"\"\n",
    "    command += \"wget --content-disposition  http://sourceforge.net/projects/mirdeepstar/files/MDS_command_line_v37.zip -P $Tools_DIR && \"\n",
    "    command += \"unzip -o $Tools_DIR/MDS_command_line_v37.zip -d $Tools_DIR && \"\n",
    "    command += \"rm $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/* && \"\n",
    "    command += \"wget --content-disposition ftp://mirbase.org/pub/mirbase/20/hairpin.fa.gz -P $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase && \"\n",
    "    command += \"pigz -p 5 -d $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/hairpin.fa.gz && \"\n",
    "    command += \"wget --content-disposition  ftp://mirbase.org/pub/mirbase/20/mature.fa.gz -P $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase && \"\n",
    "    command += \"pigz -p 5 -d $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/mature.fa.gz &&  \"\n",
    "    command += \"wget --content-disposition  ftp://mirbase.org/pub/mirbase/20/genomes/hsa.gff3 -P $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase && \"\n",
    "    command += \"mv $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/hsa.gff3 $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/knownMiR.gff3\"                \n",
    "    subprocess.call(command, shell=True)   \n",
    "    print(\"Downloading Mirdeep* is complete. Now downloading hg19 human reference genome....\")\n",
    "    ref_var = 'hg19'    \n",
    "    os.environ['REF_DIR'] = os.path.join(os.environ['HOME_DIR'], 'Tools',\n",
    "                                     'MDS_command_line_v37/MDS_command_line')\n",
    "    print(\"\")\n",
    "    command = \"\"\n",
    "    command += \"mkdir $REF_DIR/hg19 && \"\n",
    "    command += \"wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz -P $REF_DIR/hg19 && \"\n",
    "    command += \"pigz -p 5 -d $REF_DIR/hg19/hg19.fa.gz && echo Download complete. Now building bowtie indexes && \"\n",
    "    command += \"$HOME_DIR/Tools/bowtie-1.2.3-linux-x86_64/bowtie-build --threads 8 $HOME_DIR/refs/hg19/hg19.fa $HOME_DIR/refs/hg19/hg19\"\n",
    "    subprocess.call(command, shell=True)    \n",
    "    print(\"Human refenrence genome hg19 has been downloaded and index files generation complete.\")\n",
    "\n",
    "elif choice == 3:\n",
    "    print(\"You chose hg38 and miRBase v21 based genome alignment using Mirdeep*\")\n",
    "    conf_file = open(os.path.join(os.environ['Tools_DIR'],'mirdeep.conf'),\"w\")    \n",
    "    line = \"Human Genome = hg38\" + \"\\n\"\n",
    "    line += \"miRBase = 21\"\n",
    "    conf_file.write(line)\n",
    "    print(\"Downloading Mirdeep*....\")\n",
    "    command = \"\"\n",
    "    command += \"rm -rf $Tools_DIR/MDS_command_line_v38/MDS_command_line/genome/hg38 && \" \n",
    "    command += \"wget --content-disposition  http://sourceforge.net/projects/mirdeepstar/files/Index_files/hg38.zip/download -P $Tools_DIR/ && \"\n",
    "    command += \"unzip -o $Tools_DIR/hg38.zip -d $Tools_DIR/MDS_command_line_v38/MDS_command_line/genome/ && \"\n",
    "    command += \"rm -r $Tools_DIR/MDS_command_line_v38/MDS_command_line/genome/hg19\"\n",
    "    subprocess.call(command, shell=True)   \n",
    "    print(\"Downloading Mirdeep* is complete. Now downloading hg38 for piRNA pipeline...\")\n",
    "    ref_var = 'hg38'   \n",
    "    command = \"\"    \n",
    "    command += \"mkdir -p $HOME_DIR/refs/hg38 && \"\n",
    "    command += \"wget --content-disposition http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz -P $HOME_DIR/refs/hg38 && \"\n",
    "    command += \"pigz -p 5 -d -f $HOME_DIR/refs/hg38/hg38.fa.gz && echo Download complete. Now building bowtie indexes && \"\n",
    "    command += \"$HOME_DIR/Tools/bowtie-1.2.3-linux-x86_64/bowtie-build --threads 8 $HOME_DIR/refs/hg38/hg38.fa $HOME_DIR/refs/hg38/hg38\"\n",
    "    subprocess.call(command, shell=True)   \n",
    "    print(\"Human refenrence genome hg38 has been downloaded and index files generation complete.\")\n",
    "\n",
    "elif choice == 4:\n",
    "    print(\"You chose hg38 and miRBase v22 based genome alignment using Mirdeep*(Default Condition)\")\n",
    "    conf_file = open(os.path.join(os.environ['Tools_DIR'],'mirdeep.conf'),\"w\")    \n",
    "    line = \"Human Genome = hg38\" + \"\\n\"\n",
    "    line += \"miRBase = 22\"\n",
    "    conf_file.write(line)\n",
    "    ref_var = 'hg38' \n",
    "    print(\"Downloading hg38 human reference genome\")\n",
    "    command = \"\"\n",
    "    command += \"mkdir -p $HOME_DIR/refs/hg38 && \"\n",
    "    command += \"wget --content-disposition http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz -P $HOME_DIR/refs/hg38 && \"\n",
    "    command += \"pigz -p 5 -d -f $HOME_DIR/refs/hg38/hg38.fa.gz && echo Download complete. Now building bowtie indexes && \"\n",
    "    command += \"$HOME_DIR/Tools/bowtie-1.2.3-linux-x86_64/bowtie-build --threads 8 $HOME_DIR/refs/hg38/hg38.fa $HOME_DIR/refs/hg38/hg38\"\n",
    "    subprocess.call(command, shell=True)   \n",
    "    print(\"Human refenrence genome hg38 has been downloaded and index files generation complete.\")\n",
    "\n",
    "else:\n",
    "    print(\"Please enter valid option\")\n",
    "\n",
    "conf_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## piRNA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piRNA_pipeline():\n",
    "    cmd = 'bash scripts/piRNA_pipeline.sh 1> data/LOG_DIR/piRNA_pipeline.stdout 2> data/LOG_DIR/piRNA_pipeline.stderr'\n",
    "    os.system(cmd)\n",
    "\n",
    "threading.Thread(target=piRNA_pipeline).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## miRNA sequence Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following command if you want to perform sequence alignment sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequential sample processing\n",
    "# !bash $HOME_DIR/scripts/Mirdeep_star.sh 1>$LOG_DIR/Mirdeep_star.stdout 2>$LOG_DIR/Mirdeep_star.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing batches for miRNA sequence alignment for multi-thread processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirdeep_star(batch,batch_id):    \n",
    "    os.chdir(os.environ['REF_DIR'])\n",
    "    files = os.listdir(os.getcwd())    \n",
    "    ref_id = os.path.join(os.getcwd(),'genome')\n",
    "    if 'hg19' in os.listdir(ref_id):\n",
    "        ref_idx = 'hg19'\n",
    "    elif 'hg38' in os.listdir(ref_id):\n",
    "        ref_idx = 'hg38'\n",
    "    else:\n",
    "        print('Reference index are missing in ', ref_id)\n",
    "        sys.exit(2)\n",
    "    \n",
    "    batch = pd.read_csv(os.path.join(os.environ['SEQ_DIR'],batch),index_col=0)\n",
    "    for idx in range(batch.shape[0]):\n",
    "        basename = batch.iloc[idx,0].split('.')[0] +'_trimmed.fastq'\n",
    "        path_new = os.path.join(os.environ['SEQ_DIR'],'fastq_17_24',basename)\n",
    "        \n",
    "        # Constraining miRDeep* to take only 4 gb in each thread\n",
    "        try:\n",
    "            command = 'java -Xmx4096m -jar MD.jar -g '+ ref_idx + ' -a ' + os.environ['adaptor'] + ' -t 17 -l 24 -s -20 -r 5 -p 20 -m 101 ' + path_new + ' && rm ' + path_new\n",
    "            os.system(command)\n",
    "        except:\n",
    "            screenlock.acquire()\n",
    "            print('Sample ',basename,' is not present in the directory.')\n",
    "            screenlock.release()\n",
    "    \n",
    "    screenlock.acquire()\n",
    "    print('---------------------------')\n",
    "    print(batch_id, ' is complete.')\n",
    "    print('---------------------------')\n",
    "    screenlock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the sample_list provided by user\n",
    "sample_list = pd.read_csv('data/sample_list.csv',index_col=0)\n",
    "\n",
    "# Generating batches\n",
    "if sample_list.shape[0] < 10:\n",
    "    no_of_batch = 2\n",
    "elif sample_list.shape[0] > 10 and sample_list.shape[0] <= 30:\n",
    "    no_of_batch = 4\n",
    "elif sample_list.shape[0] > 30 and sample_list.shape[0] <= 50:\n",
    "    no_of_batch = 6\n",
    "elif sample_list.shape[0] > 50 and sample_list.shape[0] <= 100:\n",
    "    no_of_batch = 8\n",
    "elif sample_list.shape[0] > 100:\n",
    "    no_of_batch = 10\n",
    "    \n",
    "file_list_splitted = np.array_split(sample_list, no_of_batch)\n",
    "for batch_idx in range(len(file_list_splitted)):\n",
    "    file_list_splitted[batch_idx].to_csv('data/batch_'+str(batch_idx+1)+'.csv',encoding='utf-8',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling mirdeep* for each batch (except first) in individual threads.\n",
    "for i in range(2,no_of_batch+1):\n",
    "    print('Running batch_'+ str(i)+ ' on thread-'+ str(i))\n",
    "    threading.Thread(target=mirdeep_star,args=('batch_'+str(i)+'.csv',\n",
    "                                               'batch_'+str(i),)).start()    \n",
    "    \n",
    "# Running batch_1\n",
    "print('Running batch_1 on thread-1')\n",
    "mirdeep_star('batch_1.csv','batch_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Alignment Analysis\n",
    "## **Processing of raw counts from Mirdeep* results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.environ['HOME_DIR'])\n",
    "files = [] #['synthetic_data.result.updated']\n",
    "[files.append(i) for i in os.listdir(\"data/fastq_17_24\") if (\".result\" in i and not \"known\" in i)]\n",
    "result_data = {}\n",
    "print('Preparing master look up table for result data')\n",
    "for file in (files):    \n",
    "    result_data[file] = {}\n",
    "    readfile = open(os.path.join(\"data/fastq_17_24\", file), 'r').readlines()\n",
    "    header = readfile[0].split(\"\\t\")\n",
    "    for line in readfile[1:]:\n",
    "        result_data[file][line.split(\"\\t\")[0]] = {}\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[1]] = line.split(\"\\t\")[1]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[2]] = line.split(\"\\t\")[2]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[3]] = line.split(\"\\t\")[3]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[4]] = line.split(\"\\t\")[4]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[5]] = line.split(\"\\t\")[5]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[6]] = line.split(\"\\t\")[6]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[7]] = line.split(\"\\t\")[7]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[8]] = line.split(\"\\t\")[8]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[9]] = line.split(\"\\t\")[9]\n",
    "        result_data[file][line.split(\"\\t\")[0]][header[10]] = line.split(\"\\t\")[10]\n",
    "\n",
    "        \n",
    "print('Master lookup table is generated. Now collecting all unique miRs from master lookup table')\n",
    "mir_dict = {}\n",
    "for file in (result_data.keys()):\n",
    "    print('Working on ',file)\n",
    "    for k,v in result_data[file].items():\n",
    "        i = 1\n",
    "        if not k in mir_dict.keys():\n",
    "            mir_dict[k] = [v['chr'],v['mature_loci'].split('-')[0],\n",
    "                           v['mature_loci'].split('-')[1],v['mature miR']]\n",
    "        else:\n",
    "            mir_dict1 = mir_dict.copy()\n",
    "            for k1,v1 in mir_dict1.items():\n",
    "                new_loc_start = int(v['mature_loci'].split('-')[0])\n",
    "                new_loc_end = int(v['mature_loci'].split('-')[0])\n",
    "                new_mir_seq = v['mature miR']\n",
    "                if k1 == k:\n",
    "                    if v1[0] == v['chr']:                          \n",
    "                        if new_loc_start == int(v1[1]) and new_loc_end == int(v1[2]):\n",
    "                            if new_mir_seq == v1[3]:\n",
    "                                pass\n",
    "                    else:\n",
    "                        mir_dict[k+'_'+str(i)] = [v['chr'],v['mature_loci'].split('-')[0],\n",
    "                                       v['mature_loci'].split('-')[1],v['mature miR']]\n",
    "                        i += 1\n",
    "\n",
    "\n",
    "print('Prepating dataframe for all unique miRs and samples....')\n",
    "counts = pd.DataFrame(columns=list(result_data.keys()))\n",
    "mir_name = list(mir_dict.keys())\n",
    "index = 0\n",
    "\n",
    "for k,v in (mir_dict.items()):\n",
    "    for file in result_data.keys():\n",
    "        for k1,v1 in result_data[file].items():\n",
    "            if v1['mature miR'] == v[3]:\n",
    "                loc_start = int(v1['mature_loci'].split('-')[0])\n",
    "                loc_end = int(v1['mature_loci'].split('-')[1])\n",
    "                loc_chr = v1['chr']\n",
    "                if loc_chr == v[0]:\n",
    "                    if loc_start == int(v[1]) and loc_end == int(v[2]):   \n",
    "                        counts.loc[k,file] = int(v1['expression(number of mature reads)'])\n",
    "                   \n",
    "                        \n",
    "counts['mir_ID'] = mir_name\n",
    "counts = counts.set_index('mir_ID')\n",
    "counts['chr'] = [v[0] for v in mir_dict.values()]\n",
    "counts['chr_start'] = [v[1] for v in mir_dict.values()]\n",
    "counts['chr_end'] = [v[2] for v in mir_dict.values()]\n",
    "counts['miRNA_sequence'] = [v[3] for v in mir_dict.values()]\n",
    "\n",
    "# rearranging columns\n",
    "cols = counts.columns.tolist()\n",
    "cols = cols[-4:]+ cols[:-4]\n",
    "counts = counts[cols]\n",
    "\n",
    "counts = counts.fillna(0)\n",
    "print(counts.shape)\n",
    "counts.to_csv(\"data/counts.csv\",encoding='utf-8',index=True)\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_known = counts[counts.index.str.contains('hsa')]\n",
    "counts_unknown = counts[~counts.index.str.contains('hsa')]\n",
    "print('All %d known and %d novel miRNA are successfully separated' %(counts_known.shape[0],counts_unknown.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blast Search\n",
    "## Search for Novel miRNA sequence in DashR Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Run this cell only when running this notebook first time. After successfully running this, \n",
    "   pickle file will be saved which can be used further without generating the \n",
    "   results again and will save lot of time.\n",
    "   This code has been tested with Firefox version 66.0.4 (64-bit).\n",
    "'''\n",
    "\n",
    "log_writer = open(\"data/LOG_DIR/DashR_database_searching.txt\", 'w')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('###################################################################################\\n')\n",
    "log_writer.write('************************Starting DashR analysis*********************************\\n')\n",
    "log_writer.write('###################################################################################\\n')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('\\n')\n",
    "\n",
    "novel_mirs = list(counts_unknown.index.values)\n",
    "gecko = os.path.abspath('geckodriver')\n",
    "dashR_Results = []\n",
    "browser = Browser('firefox',executable_path=gecko,headless=True)\n",
    "browser.visit('http://dashr2.lisanwanglab.org/search.php#')\n",
    "\n",
    "if ref_var == 'hg19':\n",
    "    #  Select DASHR2 GEO HG19 as reference\n",
    "    xpath = '/html/body/div/div[1]/div/div/select/option[3]' \n",
    "else:\n",
    "    #  Select DASHR2 GEO HG38 as reference\n",
    "    xpath = '/html/body/div/div[1]/div/div/select/option[4]'\n",
    "browser.find_by_xpath(xpath).click()\n",
    "time.sleep(3)\n",
    "\n",
    "browser.find_by_text('Search by sequence ').click()\n",
    "time.sleep(3)\n",
    "n_idx = len(list(counts_unknown.loc[:,'miRNA_sequence']))\n",
    "n = 0\n",
    "for idx in tqdm(range(n_idx)):\n",
    "    name = list(counts_unknown.index.values)[idx]\n",
    "    seq = list(counts_unknown.loc[:,'miRNA_sequence'])[idx]\n",
    "    dashR_Results.append(name)     \n",
    "    log_writer.write('\\n\\nSearch for ' + str(name) + ' having sequence : ' + str(seq) + '\\n')\n",
    "    browser.fill('querySeq', seq)\n",
    "    time.sleep(2)\n",
    "    xpath = '//*[@id=\"search\"]/div[4]/div/button'    \n",
    "    browser.find_by_xpath(xpath).click()\n",
    "    if not browser.is_text_present('No matches found'):\n",
    "        n += 1\n",
    "        xpath = '//*[@id=\"sequence-results\"]/pre/table' \n",
    "        results = browser.find_by_xpath(xpath)\n",
    "        i=0\n",
    "        for search_result in results:\n",
    "            title = search_result.text.encode('utf8') \n",
    "            link = search_result[\"href\"]             \n",
    "            dashR_Results.append((title, link)) \n",
    "            i += 1   \n",
    "        log_writer.write('Total ' + str(i) + 'resutls are found in DashR Database for ' + str(name) + '\\n')\n",
    "        log_writer.write('Searching for ' + str(name) + ' novel-mirna is complete.\\n')\n",
    "    else:\n",
    "        dashR_Results.append('miRNA NOT FOUND')\n",
    "        log_writer.write('No results are found for '+ str(name) + '\\n')  \n",
    "    \n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "log_writer.write('\\n\\n***************************************************************\\n')\n",
    "log_writer.write('\\n\\nTotal number of matched novel miRNAs are '+ str(n) + '\\n')\n",
    "with open(\"data/dashR_Results.pickle\", 'wb') as handle:\n",
    "    pickle.dump(dashR_Results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "log_writer.write('\\nSearched results are saved in' + str('data/LOG_DIR' + '\\n'))\n",
    "log_writer.write('\\n\\n***************************************************************\\n')\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell if you want to load the pickle file obtained from dashR search module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#    Get DashR database searching results from saved pickle file.   \n",
    "# '''\n",
    "# import pickle\n",
    "# print('Loading saved results from DashR Database....')\n",
    "# with open(\"data/dashR_Results.pickle\", 'rb') as handle:\n",
    "#     dashR_Results = pickle.load(handle)\n",
    "# len(dashR_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DashR Results post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_check(rna_name, df1,df2,idx):\n",
    "    rna_name1 = rna_name.split(' ')[1]\n",
    "    rna_chr1 = rna_name.split(' ')[3].split(':')[0]\n",
    "    rna_chr_start1 = int(rna_type.split(' ')[3].split(':')[1].split('[')[0].split('-')[0])\n",
    "    rna_chr_end1 = int(rna_type.split(' ')[3].split(':')[1].split('[')[0].split('-')[1])\n",
    "    try:\n",
    "        rna_chr2 = df1.loc[rna_name1,'chr']\n",
    "        rna_chr_start2 = int(df1.loc[rna_name1,'chr_start'])\n",
    "        rna_chr_end2 = int(df1.loc[rna_name1,'chr_end'])\n",
    "    except:\n",
    "        rna_name1 = dashR_Results[idx]\n",
    "        rna_chr2 = df2.loc[rna_name1,'chr']\n",
    "        rna_chr_start2 = int(df2.loc[rna_name1,'chr_start'])\n",
    "        rna_chr_end2 = int(df2.loc[rna_name1,'chr_end'])\n",
    "    \n",
    "    if (rna_chr1 == rna_chr2) and (rna_chr_start1 >= rna_chr_start2 -3) and (rna_chr_end1 <= rna_chr_end2 + 3):\n",
    "        flag = True\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_writer = open(\"data/LOG_DIR/DashR_results_processing.txt\", 'w')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('###################################################################################\\n')\n",
    "log_writer.write('************************Starting DashR Results Analysis****************************\\n')\n",
    "log_writer.write('###################################################################################\\n')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('\\n')\n",
    "\n",
    "\n",
    "miRNA_count = 0\n",
    "novel_contains_miRNA = []\n",
    "novel_miRNA1 = counts_unknown.copy()\n",
    "counts_unknown2 = counts_unknown.copy()\n",
    "counts_unknown2 = counts_unknown2.drop(['miRNA_sequence'],axis=1)\n",
    "counts_unknown2 = counts_unknown2.drop(['chr'],axis=1)\n",
    "counts_unknown2 = counts_unknown2.drop(['chr_start'],axis=1)\n",
    "counts_unknown2 = counts_unknown2.drop(['chr_end'],axis=1)\n",
    "counts_known2 = counts_known.copy()\n",
    "counts_known2 = counts_known2.drop(['miRNA_sequence'],axis=1)\n",
    "counts_known2 = counts_known2.drop(['chr'],axis=1)\n",
    "counts_known2 = counts_known2.drop(['chr_start'],axis=1)\n",
    "counts_known2 = counts_known2.drop(['chr_end'],axis=1)\n",
    "old_seq = list(counts_known.loc[:,'miRNA_sequence'].values)\n",
    "old_chr = list(counts_known.loc[:,'chr'].values)\n",
    "old_chr_start = list(counts_known.loc[:,'chr_start'].values)\n",
    "old_chr_end = list(counts_known.loc[:,'chr_end'].values)\n",
    "add_index_entry = list(counts_known.index.values)\n",
    "novel_name1 = []\n",
    "similar_known_mirna = []\n",
    "mirna_updated = 0\n",
    "for idx in (range(0,len(dashR_Results),2)):\n",
    "    results = dashR_Results[idx+1]    \n",
    "    if isinstance((dashR_Results[idx+1]),tuple):\n",
    "        results = list(dashR_Results[idx+1])[0].decode(\"utf-8\")          \n",
    "        miRNA_count += 1\n",
    "        rna_type_results = results.split('\\n')[1:]\n",
    "        for rna_type in rna_type_results:\n",
    "            rna_type1 = rna_type.split(' ')[1]\n",
    "            if 'hsa' in rna_type1:   \n",
    "                similar_known_mirna.append(rna_type1)            \n",
    "                idx2 = list(counts_unknown2.index.values).index(dashR_Results[idx])\n",
    "                count_to_add = list(map(int,list(counts_unknown2.iloc[idx2,:].values)))                \n",
    "                if rna_type1 in list(counts_known.index.values):    \n",
    "                    counts_add_flag = loc_check(rna_type,counts_known,counts_unknown,idx)\n",
    "                    if counts_add_flag:\n",
    "                        mirna_updated += 1\n",
    "                        count_increament = list(counts_known2.loc[rna_type1,:].values)                        \n",
    "                        count_increament = list(map(add,count_increament,count_to_add))               \n",
    "                        counts_known2.iloc[list(counts_known2.index.values).index(rna_type1),:] = count_increament                        \n",
    "                        novel_miRNA1 = novel_miRNA1.drop([str(dashR_Results[idx])],axis = 0)\n",
    "                        print(dashR_Results[idx], ' has been added to the known miR count matrix. ')\n",
    "                        log_entry = dashR_Results[idx] + ' has been added to the known miR count matrix. '\n",
    "                        log_writer.write(log_entry + '\\n')\n",
    "                    else:\n",
    "                        log_entry ='NOT UPDATED: ' + dashR_Results[idx] + ' belongs to the same family of ' + rna_type1 + ' but their genomic locations are different.'\n",
    "                        log_writer.write(log_entry + '\\n')                        \n",
    "                else:\n",
    "                    counts_add_flag = loc_check(rna_type,counts_known,counts_unknown,idx)\n",
    "                    if counts_add_flag:\n",
    "                        count_unknown_idx = list(counts_unknown2.index.values).index(str(dashR_Results[idx]))\n",
    "                        counts_known2 = counts_known2.append(counts_unknown2.iloc[count_unknown_idx,:]) \n",
    "                        add_index_entry.append(rna_type1)\n",
    "                        old_seq.append(counts_unknown.loc[str(dashR_Results[idx]),'miRNA_sequence'])\n",
    "                        old_chr.append(counts_unknown.loc[str(dashR_Results[idx]),'chr'])\n",
    "                        old_chr_start.append(counts_unknown.loc[str(dashR_Results[idx]),'chr_start'])\n",
    "                        old_chr_end.append(counts_unknown.loc[str(dashR_Results[idx]),'chr_end'])\n",
    "                        novel_miRNA1 = novel_miRNA1.drop([str(dashR_Results[idx])],axis = 0)\n",
    "                        print('ADDED: New mir added')\n",
    "                        log_entry ='ADDED: New mir added  = ' + dashR_Results[idx]\n",
    "                        log_writer.write(log_entry + '\\n')                        \n",
    "                    else:                        \n",
    "                        log_entry ='NOT ADDEDD: ' + dashR_Results[idx] + ' belongs to the same family of ' + rna_type1 + ' but their genomic locations are different.'\n",
    "                        log_writer.write(log_entry + '\\n')                        \n",
    "                break\n",
    "            else:\n",
    "                log_entry = 'Novelmir '+str(dashR_Results[idx])+' is found as : '+ rna_type1 + '. Hence it will be not considered'\n",
    "                log_writer.write(log_entry + '\\n')\n",
    "    else:\n",
    "        log_entry = 'Novelmir '+str(dashR_Results[idx])+' is not mached with any known miRNA in DashR database.'\n",
    "        log_writer.write(log_entry + '\\n')\n",
    "            \n",
    "\n",
    "counts_known2['chr'] = old_chr\n",
    "counts_known2['chr_start'] = old_chr_start\n",
    "counts_known2['chr_end'] = old_chr_end\n",
    "counts_known2['miRNA_sequence'] = old_seq\n",
    "\n",
    "# rearranging columns\n",
    "cols = counts_known2.columns.tolist()\n",
    "cols = cols[-4:]+ cols[:-4]\n",
    "counts_known2 = counts_known2[cols]\n",
    "\n",
    "counts_known2.index = add_index_entry\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_unknown2.shape)\n",
    "print(novel_miRNA1.shape)\n",
    "print(counts_known2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed based Sequence Clustering\n",
    "## **Dictionary Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict = {}\n",
    "Xseed_dict = {}\n",
    "known_mir_dict = {}\n",
    "for index in counts_known2.index:\n",
    "    seed = counts_known2.loc[index,'miRNA_sequence'][1:7]\n",
    "    Xseed = counts_known2.loc[index,'miRNA_sequence'][7:]\n",
    "    known_mir_dict[index] = [seed,counts_known2.loc[index,'chr'],counts_known2.loc[index,'chr_start'],\n",
    "                              counts_known2.loc[index,'chr_end'],counts_known2.loc[index,'miRNA_sequence']]\n",
    "    seed_dict[index] = seed\n",
    "    Xseed_dict[index] = Xseed\n",
    "    \n",
    "\n",
    "unknown_mir_dict = {}\n",
    "for index in novel_miRNA1.index:\n",
    "    seed = novel_miRNA1.loc[index,'miRNA_sequence'][1:7]\n",
    "    Xseed = novel_miRNA1.loc[index,'miRNA_sequence'][7:]\n",
    "    unknown_mir_dict[index] = [seed,novel_miRNA1.loc[index,'chr'],novel_miRNA1.loc[index,'chr_start'],\n",
    "                              novel_miRNA1.loc[index,'chr_end'],novel_miRNA1.loc[index,'miRNA_sequence']]\n",
    "    seed_dict[index] = seed\n",
    "    Xseed_dict[index] = Xseed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fasta File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "writer = open(\"data/seed_seqs.fasta\", 'w')\n",
    "for key in known_mir_dict.keys():\n",
    "    line = \">\" + str(count) + \"\\t\" + key + \"\\n\"\n",
    "    line += str(known_mir_dict[key][0]) + \"\\n\"\n",
    "    writer.write(line)\n",
    "    count+=1\n",
    "\n",
    "for key in unknown_mir_dict.keys():\n",
    "    line = \">\" + str(count) + \"\\t\" + key + \"\\n\"\n",
    "    line += str(unknown_mir_dict[key][0]) + \"\\n\"\n",
    "    writer.write(line)\n",
    "    count+=1\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "writer = open(\"data/seed_outer_seqs.fasta\", 'w')\n",
    "for key in known_mir_dict.keys():\n",
    "    line = \">\" + str(count) + \"\\t\" + key + \"\\n\"\n",
    "    line += str(known_mir_dict[key][-1][7:]) + \"\\n\"\n",
    "    writer.write(line)\n",
    "    count+=1\n",
    "\n",
    "for key in unknown_mir_dict.keys():\n",
    "    line = \">\" + str(count) + \"\\t\" + key + \"\\n\"\n",
    "    line += str(unknown_mir_dict[key][-1][7:]) + \"\\n\"\n",
    "    writer.write(line)\n",
    "    count+=1\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD-HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute CD-HIT command\n",
    "\n",
    "!cd Tools/cd-hit-v4.6.8-2017-1208/; echo \"***** Clustering miRs Seed Sequences Only**********\"; \\\n",
    "                                                           ./cd-hit -l 5 -n 2 -c 1 \\\n",
    "                                                                    -i ../../data/seed_seqs.fasta \\\n",
    "                                                                    -o ../../data/cdhit_seed;     \\\n",
    "                                                           echo \" \"; \\\n",
    "                                                           echo \"***** Clustering miRs Rest Sequences Only**********\"; \\\n",
    "                                                           ./cd-hit -l 8 -i ../../data/seed_outer_seqs.fasta \\\n",
    "                                                                    -o ../../data/cdhit_seed_outer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing lookup table for Xseed Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CLuster dictionary preparation\n",
    "Cluster : cluster_no:[mir1,mir2]\n",
    "\"\"\"\n",
    "def fetch_id(xseed_id_fasta, xid):\n",
    "    for line in xseed_id_fasta:\n",
    "        if line:\n",
    "            if '>' in line[0]:   \n",
    "                if xid == line.split('\\t')[0].split('>')[1]:\n",
    "                    return line.split('\\t')[1]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cluster_dict_preparation(cluster_file, fasta_file):\n",
    "    cluster_file = open(cluster_file,'r').read().split('\\n')\n",
    "    fasta_id = open(fasta_file,'r').read().split('\\n')\n",
    "    cluster_dict = {}    \n",
    "    for line in cluster_file:\n",
    "        if line:\n",
    "            if '>' in line[0]:\n",
    "                line_no = cluster_file.index(line)\n",
    "                flag = True    \n",
    "                k = line[1:].replace(' ','_')\n",
    "                xid = cluster_file[line_no+1].split('>')[1].split('.')[0]\n",
    "                cluster_dict[k] = [fetch_id(fasta_id,xid)]\n",
    "                i = 2\n",
    "                while flag:\n",
    "                    if cluster_file[line_no+i]:\n",
    "                        if '>' in cluster_file[line_no+i][0]:\n",
    "                            flag = False\n",
    "                            pass\n",
    "                        else:\n",
    "                            xid = cluster_file[line_no+i].split('>')[1].split('.')[0]\n",
    "                            cluster_dict[k].append(fetch_id(fasta_id,xid))\n",
    "                            i += 1\n",
    "                    else:\n",
    "                        flag = False\n",
    "                        \n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "seed_cluster = cluster_dict_preparation('data/cdhit_seed.clstr','data/seed_seqs.fasta')\n",
    "Xseed_cluster = cluster_dict_preparation('data/cdhit_seed_outer.clstr','data/seed_outer_seqs.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Only for outer seed region clusters\n",
    "rev_dict : mir_name:[cluster1,cluster2]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rev_dictionary(cluster):\n",
    "    rev_dict = defaultdict(list)\n",
    "    mir_list = list(known_mir_dict.keys())\n",
    "    for k in list(unknown_mir_dict.keys()):\n",
    "        mir_list.append(k)\n",
    "        \n",
    "    for mir in mir_list:\n",
    "        for k,v in cluster.items():\n",
    "            for mir1 in v:\n",
    "                if mir == mir1:\n",
    "                    rev_dict[mir].append(k)\n",
    "                    \n",
    "    return dict(rev_dict)\n",
    "\n",
    "\n",
    "exact_seed_rev_dict = rev_dictionary(exact_seed_cluster)\n",
    "seed_rev_dict = rev_dictionary(seed_cluster)\n",
    "Xseed_rev_dict = rev_dictionary(Xseed_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to update and merge the counts if all conditions (seed,seq and loc) are satisfied\n",
    "Input : df1,df2,mir1 (from df1), mir2 (from df2)\n",
    "Output : df1_modified (with updated counts of mir1) and df2_modified (with mir2 removed)\n",
    "\"\"\"\n",
    "\n",
    "def update_counts(df1,df2,mir1,mir2):\n",
    "    try:\n",
    "        mir1_counts = list(df1.loc[mir1,:].values)[4:]\n",
    "        mir2_counts = list(df2.loc[mir2,:].values)[4:]\n",
    "        mir1_counts = list(map(add,mir1_counts,mir2_counts))\n",
    "        df1.loc[mir1,:] = list(df1.loc[mir1,:].values)[:4] + mir1_counts\n",
    "        df2 = df2.drop(mir2,axis = 0)    \n",
    "        return df1, df2\n",
    "    except:\n",
    "        print('Either mir1 or mir2 has been removed.')\n",
    "        return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_writer = open(\"data/LOG_DIR/seed_based_clustering.txt\", 'w')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('###################################################################################\\n')\n",
    "log_writer.write('************************Starting Seed-based Clustering****************************\\n')\n",
    "log_writer.write('###################################################################################\\n')\n",
    "log_writer.write('\\n')\n",
    "log_writer.write('\\n')\n",
    "\n",
    "counts_known3 = counts_known2.copy()\n",
    "novel_miRNA2 = novel_miRNA1.copy()\n",
    "mir_reannotation = pd.DataFrame()\n",
    "mir_old_name = []\n",
    "mir_new_name = []\n",
    "novel_mir_seq = []\n",
    "family_name = []\n",
    "attribute = []\n",
    "\n",
    "for k,v in tqdm(seed_cluster.items()):    \n",
    "    novel_flag = True\n",
    "    novel_mir_name = []\n",
    "    known_mir_name = []\n",
    "    for mir in seed_cluster[k]:\n",
    "        if not 'hsa' in mir:\n",
    "            novel_mir_name.append(mir)\n",
    "            novel_flag = False\n",
    "        else:\n",
    "            known_mir_name.append(mir)\n",
    "            \n",
    "    if not novel_flag:\n",
    "        char_no = 0\n",
    "        for n_mir in novel_mir_name:\n",
    "            n_chr_loc = unknown_mir_dict[n_mir][1]\n",
    "            n_chr_loc_start = int(unknown_mir_dict[n_mir][2])\n",
    "            n_chr_loc_end = int(unknown_mir_dict[n_mir][3])\n",
    "            if known_mir_name:\n",
    "                for k_mir in known_mir_name:\n",
    "                    novel_mir_name = [i for i in novel_mir_name if i is not None]\n",
    "                    if novel_mir_name and n_mir in novel_mir_name:                        \n",
    "                        k_chr_loc = known_mir_dict[k_mir][1]\n",
    "                        k_chr_loc_start = int(known_mir_dict[k_mir][2])\n",
    "                        k_chr_loc_end = int(known_mir_dict[k_mir][3])\n",
    "                        if k_mir in Xseed_cluster[Xseed_rev_dict[n_mir][0]]:\n",
    "                            if (n_chr_loc == k_chr_loc) and (n_chr_loc_start == k_chr_loc_start) and (n_chr_loc_end == k_chr_loc_end):\n",
    "                                counts_known3,novel_miRNA2 = update_counts(counts_known3,novel_miRNA2,k_mir,n_mir)\n",
    "                                log_entry = n_mir + '\\t' + unknown_mir_dict[n_mir][-1] + '\\t' + n_chr_loc + 't' + str(n_chr_loc_start) + '\\t' + str(n_chr_loc_end)\n",
    "                                log_writer.write(log_entry + '\\n')                                            \n",
    "                                log_entry = k_mir + '\\t' + known_mir_dict[k_mir][-1] + '\\t' + k_chr_loc + 't' + str(k_chr_loc_start) + '\\t' + str(k_chr_loc_end)\n",
    "                                log_writer.write(log_entry + '\\n')                                            \n",
    "                                log_entry = n_mir + ' will be merged to ' + k_mir\n",
    "                                log_writer.write(log_entry + '\\n')                                            \n",
    "                                novel_mir_name[novel_mir_name.index(n_mir)] = None \n",
    "                                mir_old_name.append(n_mir)\n",
    "                                mir_new_name.append(k_mir)\n",
    "                                family_name.append(k_mir)\n",
    "                                novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                attribute.append('Same Seed, Xseed and loc')\n",
    "                                break\n",
    "                            else:\n",
    "                                if char_no < 26:\n",
    "                                    new_name = k_mir+'-'+ chr(97 + char_no)\n",
    "                                else:\n",
    "                                    a,b,c = random.randint(0,25), random.randint(0,25), random.randint(0,25)\n",
    "                                    new_name = k_mir+'-'+ chr(97 + a) + chr(97 + b) + chr(97 + c)\n",
    "                                log_entry = n_mir + '\\t' + unknown_mir_dict[n_mir][-1] + '\\t' + n_chr_loc + '\\t' + str(n_chr_loc_start) + '\\t' + str(n_chr_loc_end)\n",
    "                                log_writer.write(log_entry + '\\n')                                            \n",
    "                                log_entry = k_mir + '\\t' + known_mir_dict[k_mir][-1] + '\\t' + k_chr_loc + '\\t' + str(k_chr_loc_start) + '\\t' + str(k_chr_loc_end)\n",
    "                                log_writer.write(log_entry + '\\n')\n",
    "                                log_entry = n_mir + ' will be renamed as ' + new_name\n",
    "                                log_writer.write(log_entry + '\\n')                                    \n",
    "                                novel_miRNA2 = novel_miRNA2.rename(index={n_mir : new_name})\n",
    "                                novel_mir_name[novel_mir_name.index(n_mir)] = None\n",
    "                                char_no += 1\n",
    "                                mir_old_name.append(n_mir)\n",
    "                                mir_new_name.append(new_name)\n",
    "                                family_name.append(k_mir)\n",
    "                                novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                attribute.append('Functionally Same')\n",
    "                                break\n",
    "                        else:\n",
    "                            log_entry = n_mir + '\\t' + unknown_mir_dict[n_mir][-1] + '\\t' + n_chr_loc + '\\t' + str(n_chr_loc_start) + '\\t' + str(n_chr_loc_end)\n",
    "                            log_writer.write(log_entry + '\\n')                                            \n",
    "                            log_entry = k_mir + '\\t' + known_mir_dict[k_mir][-1] + '\\t' + k_chr_loc + '\\t' + str(k_chr_loc_start) + '\\t' + str(k_chr_loc_end)\n",
    "                            log_writer.write(log_entry + '\\n')\n",
    "                            log_entry = n_mir + ' will be treated as pure novel miR with same phylogenetic tree member of ' + k_mir + ' family'\n",
    "                            log_writer.write(log_entry + '\\n')\n",
    "                            novel_mir_name[known_mir_name.index(k_mir)] = None\n",
    "                            mir_old_name.append(n_mir)\n",
    "                            mir_new_name.append(n_mir)\n",
    "                            family_name.append(k_mir)\n",
    "                            novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                            attribute.append('Same Phylogenetic Tree')\n",
    "                            break\n",
    "                                \n",
    "                                \n",
    "                        \n",
    "            \n",
    "        novel_mir_name = [i for i in novel_mir_name if i is not None]                        \n",
    "        if len(novel_mir_name)>1:  \n",
    "            for n_mir in novel_mir_name:\n",
    "                if not n_mir == None:\n",
    "                    char_no = 1\n",
    "                    novel_mir_name1 = novel_mir_name.copy()\n",
    "                    novel_mir_name1.remove(n_mir)\n",
    "                    n_chr_loc = unknown_mir_dict[n_mir][1]\n",
    "                    n_chr_loc_start = int(unknown_mir_dict[n_mir][2])\n",
    "                    n_chr_loc_end = int(unknown_mir_dict[n_mir][3])\n",
    "                    seed_common_mir = list(set(novel_mir_name1) & set(exact_seed_cluster[exact_seed_rev_dict[n_mir][0]]))\n",
    "                    if seed_common_mir:                    \n",
    "                        Xseed_common_mir = list(set(seed_common_mir) & set(Xseed_cluster[Xseed_rev_dict[n_mir][0]]))\n",
    "                        if Xseed_common_mir:\n",
    "                            for c_mir1 in Xseed_common_mir:\n",
    "                                c_chr_loc = unknown_mir_dict[c_mir1][1]\n",
    "                                c_chr_loc_start = int(unknown_mir_dict[c_mir1][2])\n",
    "                                c_chr_loc_end = int(unknown_mir_dict[c_mir1][3])\n",
    "                                if (n_chr_loc == c_chr_loc) and (n_chr_loc_start == c_chr_loc_start) and (n_chr_loc_end == c_chr_loc_end):\n",
    "                                    log_entry = n_mir + '\\t' + unknown_mir_dict[n_mir][-1] + '\\t' + n_chr_loc + '\\t' + str(n_chr_loc_start) + '\\t' + str(n_chr_loc_end)\n",
    "                                    log_writer.write(log_entry + '\\n')                                            \n",
    "                                    log_entry = c_mir1 + '\\t' + unknown_mir_dict[c_mir1][-1] + '\\t' + c_chr_loc + '\\t' + str(c_chr_loc_start) + '\\t' + str(c_chr_loc_end)\n",
    "                                    log_writer.write(log_entry + '\\n')\n",
    "                                    log_entry = n_mir + ' will be merged to ' + c_mir1\n",
    "                                    log_writer.write(log_entry + '\\n')                                    \n",
    "                                    _,novel_miRNA2 = update_counts(novel_miRNA2,novel_miRNA2,n_mir,c_mir1)\n",
    "                                    novel_mir_name[novel_mir_name.index(c_mir1)] = None\n",
    "                                    mir_old_name.append(n_mir)\n",
    "                                    mir_new_name.append(c_mir1)\n",
    "                                    family_name.append(c_mir1)\n",
    "                                    novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                    attribute.append('Same Seed, Xseed and loc')\n",
    "                                    break\n",
    "                                else:\n",
    "                                    if char_no < 26:\n",
    "                                        new_name = n_mir+'-'+ chr(97 + char_no)\n",
    "                                    else:\n",
    "                                        a,b,c = random.randint(0,25), random.randint(0,25), random.randint(0,25)\n",
    "                                        new_name = n_mir+'-'+ chr(97 + a) + chr(97 + b) + chr(97 + c)\n",
    "                                    log_entry = n_mir + '\\t' + unknown_mir_dict[n_mir][-1] + '\\t' + n_chr_loc + '\\t' + str(n_chr_loc_start) + '\\t' + str(n_chr_loc_end)\n",
    "                                    log_writer.write(log_entry + '\\n')                                            \n",
    "                                    log_entry = c_mir1 + '\\t' + unknown_mir_dict[c_mir1][-1] + '\\t' + c_chr_loc + '\\t' + str(c_chr_loc_start) + '\\t' + str(c_chr_loc_end)\n",
    "                                    log_writer.write(log_entry + '\\n')\n",
    "                                    log_entry = n_mir + ' and ' + c_mir1 + ' are functionally same. So ' + c_mir1 + ' will be renamed to ' + new_name\n",
    "                                    log_writer.write(log_entry + '\\n')                                    \n",
    "                                    novel_miRNA2 = novel_miRNA2.rename(index={c_mir1 : new_name})\n",
    "                                    novel_mir_name[novel_mir_name.index(c_mir1)] = None\n",
    "                                    char_no += 1\n",
    "                                    mir_old_name.append(c_mir1)\n",
    "                                    mir_new_name.append(new_name)\n",
    "                                    family_name.append(n_mir)\n",
    "                                    novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                    attribute.append('Functionally Same')\n",
    "                                    break\n",
    "                        else:\n",
    "                            for c_mir1 in seed_common_mir:                                \n",
    "                                log_entry = n_mir + '\\t' + unknown_mir_dict[n_mir][-1] + '\\t' + n_chr_loc + '\\t' + str(n_chr_loc_start) + '\\t' + str(n_chr_loc_end)\n",
    "                                log_writer.write(log_entry + '\\n')                                            \n",
    "                                log_entry = c_mir1 + '\\t' + unknown_mir_dict[c_mir1][-1] + '\\t' + c_chr_loc + '\\t' + str(c_chr_loc_start) + '\\t' + str(c_chr_loc_end)\n",
    "                                log_writer.write(log_entry + '\\n')\n",
    "                                log_entry = n_mir + ' and ' + c_mir1 + ' belongs to same family of phylogenetic tree.'\n",
    "                                log_writer.write(log_entry + '\\n')                                     \n",
    "                                char_no += 1\n",
    "                                novel_mir_name[novel_mir_name.index(c_mir1)] = None\n",
    "                                mir_old_name.append(c_mir1)\n",
    "                                mir_new_name.append(c_mir1)\n",
    "                                family_name.append(n_mir)\n",
    "                                novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                attribute.append('Same Phylogenetic Tree')\n",
    "                                                            \n",
    "            \n",
    "mir_reannotation['Old_miR_ID'] = mir_old_name\n",
    "mir_reannotation['New_miR_ID'] = mir_new_name\n",
    "mir_reannotation['Family'] = family_name\n",
    "mir_reannotation['Relation'] = attribute\n",
    "mir_reannotation['sequence'] = novel_mir_seq\n",
    "mir_reannotation.to_csv(\"data/mir_reannotation.csv\",encoding='utf-8',index=False)\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from functional annotation module are saved in **data/mir_reannotation.csv** and processing details are saved in **data/LOG_DIR/seed_based_clustering.txt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = [counts_known2,novel_miRNA2]\n",
    "counts_final = pd.concat(frame)\n",
    "counts_final.to_csv(\"data/counts_final.csv\",encoding='utf-8',index=True)\n",
    "print(counts_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the final count matrix for differential expression analysis\n",
    "counts_final = counts_final.reindex(sorted(counts_final.columns), axis=1)\n",
    "counts_final = counts_final.drop(['chr','chr_start','chr_end','miRNA_sequence'],axis=1)\n",
    "counts_final.to_csv(\"data/counts_final_1.csv\",encoding='utf-8',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# miRNA Differential expression Analysis using DESeq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:24:40.850010Z",
     "start_time": "2019-05-22T10:24:30.691078Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Before reunning this module, please confirm that the order of sample \n",
    "    written in sample_list.csv and order of columns in counts_final_1.csv\n",
    "    is same.e.g.\n",
    "    In sample_list.csv the samples are:\n",
    "    Sample        File        Condition\n",
    "    SampleA   sampleA.fastq    treated\n",
    "    SampleB   sampleB.fastq    treated\n",
    "    SampleC   sampleC.fastq    treated\n",
    "    SampleD   sampleD.fastq    treated\n",
    "       .           .              .   \n",
    "       .           .              .   \n",
    "       .           .              .   \n",
    "       .           .              .   \n",
    "       \n",
    "    The column order in counts_final_1.csv should be:\n",
    "    \n",
    "       .    sampleA    sampleB    sampleC    sampleD    \n",
    "       .      25         102         10         5\n",
    "       .       .          .           .         .\n",
    "       .       .          .           .         .\n",
    "       .       .          .           .         .\n",
    "\"\"\"\n",
    "# !Rscript $HOME_DIR/scripts/install_packages.R\n",
    "\n",
    "!Rscript $HOME_DIR/scripts/norm_diff_exp.R miRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# piRNA Counts Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['COUNT_DIR'] = os.path.join(os.environ['HOME_DIR'],'data','piRNA','pirna_counts')   \n",
    "# os.chdir(os.environ['COUNT_DIR'])\n",
    "\n",
    "raw_counts = pd.DataFrame()\n",
    "for file in tqdm(os.listdir(os.environ['COUNT_DIR'])):\n",
    "    if file.endswith('_counts.txt'):\n",
    "        file_open = open(os.path.join(os.environ['COUNT_DIR'],file),\"r\").read().split(\"\\n\")        \n",
    "        pi_name = []\n",
    "        pi_count = []\n",
    "        for line in file_open:\n",
    "            if len(line) > 1:\n",
    "                pi_name.append(line.split(\"\\t\")[-2])\n",
    "                pi_count.append(line.split(\"\\t\")[-1])\n",
    "        raw_counts[file.split(\"_\")[0]] = pi_count\n",
    "        \n",
    "\n",
    "raw_counts[\"piRNA_ID\"] = pi_name\n",
    "raw_counts = raw_counts.set_index([\"piRNA_ID\"])\n",
    "\n",
    "raw_counts = raw_counts.apply(pd.to_numeric)\n",
    "raw_counts = raw_counts.groupby(['piRNA_ID']).sum()\n",
    "\n",
    "\n",
    "# Count filtering : Removing those piRNA's which are not expressed in any sample\n",
    "raw_counts1 = pd.DataFrame(columns = raw_counts.columns)\n",
    "for index in list(raw_counts.index.values):\n",
    "    if sum([int(val) for val in raw_counts.loc[index,:].values]) == 0:\n",
    "        next\n",
    "    else:\n",
    "        raw_counts1.loc[index,:] = [int(val) for val in raw_counts.loc[index,:].values]\n",
    "        \n",
    "# Sorting columns\n",
    "raw_counts1 = raw_counts1.reindex(sorted(raw_counts1.columns), axis=1)\n",
    "# Saving Results\n",
    "raw_counts1.to_csv(os.path.join(os.environ['HOME_DIR'],'data','piRNA','piRNA_raw_counts.csv'),encoding='utf-8',index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# piRNA Differential expression Analysis using DESeq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Rscript $HOME_DIR/scripts/norm_diff_exp.R piRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Finding significantly altered differentially expressed miRNAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:52:01.256168Z",
     "start_time": "2019-05-20T12:52:00.998631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This module will save counts log2fold change and p_adj value of segnificantly expressed miRNAs found in  \n",
    "    previous step.\n",
    "'''\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(os.environ['SEQ_DIR'],'miRNA_significantly_DE_mir.csv'), sep=',',header=None)\n",
    "df2 = pd.read_csv(os.path.join(os.environ['SEQ_DIR'],'counts_final.csv'), sep=',',header=0)\n",
    "try:    \n",
    "    df2 = df2.set_index(['mirID'])\n",
    "except:\n",
    "    a = df2[df2.columns[df2.columns.str.startswith('Unnamed:')]]\n",
    "    df2['mirID'] = list(a.loc[:,list(a.columns.values)[0]].values)\n",
    "\n",
    "    # Reomove Unnamed column\n",
    "    df2 = df2[df2.columns[~df2.columns.str.startswith('Unnamed:')]]\n",
    "    df2 = df2.set_index(['mirID'])\n",
    "df3 = pd.DataFrame()\n",
    "for idx in range(df1.shape[0]):\n",
    "    idx2 = list(df2.index.values).index(df1.iloc[idx,0])   \n",
    "    df3 = df3.append(df2.iloc[idx2,:])\n",
    "\n",
    "# rearranging columns\n",
    "cols = df3.columns.tolist()\n",
    "cols = cols[-4:]+ cols[:-4]\n",
    "df3 = df3[cols]\n",
    "# Saving counts of significantly differentially exoressed miRNA\n",
    "\n",
    "df3.to_csv(os.path.join(os.environ['SEQ_DIR'],'miRNA_DEmir_Counts.csv'),encoding='utf-8',index=True)\n",
    "\n",
    "# Add expression details in above results\n",
    "\n",
    "df4 = pd.read_csv(os.path.join(os.environ['SEQ_DIR'],\"miRNA_DESeq_expression_data_controlIsUnreated.csv\"),sep=',',header=0,index_col=0)\n",
    "df4 = df4.set_index(['cts...1.'])\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "p_adj = []\n",
    "up_down_expr = []\n",
    "fold_change = []\n",
    "SE_mir = list(df3.index.values)\n",
    "for mir in SE_mir:    \n",
    "    idx = list(df4.index.values).index(mir)\n",
    "    p_adj_val = \"{0:.5f}\".format(df4.loc[str(mir),'padj'])\n",
    "    p_adj.append(p_adj_val)\n",
    "    if df4.loc[str(mir),'log2FoldChange']>0:\n",
    "        up_down_expr_val = 'up'\n",
    "    else:\n",
    "        up_down_expr_val = 'down'\n",
    "        \n",
    "    up_down_expr.append(up_down_expr_val)\n",
    "    fold_change.append(df4.loc[str(mir),'log2FoldChange'])\n",
    "        \n",
    "df5['miRNA ID'] = SE_mir\n",
    "df5['p_adj'] = p_adj\n",
    "df5['Regulation'] = up_down_expr\n",
    "df5['Fold Change'] = fold_change\n",
    "df5.to_csv(os.path.join(os.environ['SEQ_DIR'],'miRNA_DEmir_Res.tsv'),sep='\\t',encoding='utf-8',index=True)\n",
    "print(df5.shape)\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "You can check your results in the following files:\n",
    "\n",
    "    1. miRNA_significantly_DE_mir.csv : This file contains the list of significantly differentially expressed miRNAs (Location: In the same directory where all fastq files are stored).\n",
    "    \n",
    "    2. miRNA_DEmir_Res.tsv  : This file contains significantly differentially expressed miRNA along with p_adj values (Location: In the same directory where all fastq files are stored).\n",
    "    \n",
    "    3. miRNA_DEmir_Counts.tsv : This file contains counts of significantly differentially expressed miRNAs in all samples (Location: In the same directory where all fastq files are stored).\n",
    "    \n",
    "    4. mir_reannotation.csv : This file contains results from functional annotation modules (Location: In the same directory where all fastq files are stored).\n",
    "    \n",
    "    5. counts_final_1 : This file contains all miRNAs raw counts obtained form all samples before differential expression analysis.\n",
    "    \n",
    "    6. piRNA_raw_counts.csv :  This file contains raw counts of all piRNAs ontained from all samples before differential expression analysis (Location: piRNA/piRNA_raw_counts.csv).\n",
    "    \n",
    "    7. piRNA_significantly_DE_mir.csv : This file contains the list  of all significantly differentially expressed piRNAs (Location: piRNA/piRNA_significantly_DE_mir.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Open source RNA-Seq pipeline for identification of novel-mirs and their gene regulatory networks",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
