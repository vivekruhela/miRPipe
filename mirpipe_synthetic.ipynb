{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiRPipe Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Figures/miRPipe_synthetic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastQ Files\n",
    "**Loading libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.057874Z",
     "start_time": "2019-05-20T12:51:43.698812Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import subprocess\n",
    "import random\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time\n",
    "import networkx as nx\n",
    "from operator import add\n",
    "from os import path\n",
    "import threading\n",
    "import plotly.graph_objects as go\n",
    "import subprocess\n",
    "import tempfile\n",
    "from rna_tools.Seq import RNASequence\n",
    "from rna_tools.rna_tools_config import RFAM_DB_PATH\n",
    "from threading import Semaphore\n",
    "screenlock = Semaphore(value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.121790Z",
     "start_time": "2019-05-20T12:51:49.062252Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declaring ENV Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.151397Z",
     "start_time": "2019-05-20T12:51:49.135554Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting env variables:\n",
    "os.environ['HOME_DIR'] = os.getcwd()\n",
    "os.environ['SEQ_DIR'] = os.path.join(os.environ['HOME_DIR'],'data')\n",
    "os.environ['Tools_DIR'] = os.path.join(os.environ['HOME_DIR'],'Tools')    \n",
    "os.environ['REF_DIR'] = os.path.join(os.environ['HOME_DIR'], 'Tools',\n",
    "                                     'MDS_command_line_v38/MDS_command_line')\n",
    "os.mkdir(os.path.join(os.environ['SEQ_DIR'],'output'))\n",
    "print('Please be sure that all the fastq files are present in %s. All the results will be saved in the same folder also.' %os.environ['SEQ_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that path is correct and fastq files are present in this path\n",
    "For Smooth Functioning of Pipeline, please make sure that only input fastq files are present in the data folder.\n",
    "Delete all the clutter before re-running the pipeline for trouble-free execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:51:49.634658Z",
     "start_time": "2019-05-20T12:51:49.156447Z"
    }
   },
   "outputs": [],
   "source": [
    "!echo $HOME_DIR\n",
    "!echo $SEQ_DIR\n",
    "!echo $REF_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "## Quality Check of Fastq Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Following script perform the following task:\n",
    "    1. Perform quality checking of rach samples using FastQC.\n",
    "    2. Prepare the better info-graphic and detailed report of all quality checking\n",
    "       reports from all samples using multiqc\n",
    "'''\n",
    "!Rscript $HOME_DIR/scripts/FastQC.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptor Timming and Fastq Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The default adaptor sequence used for adaptor trimming is \\\n",
    "TCGTATGCCGTCTTCTGCTTG. If your adaptor sequence is different then please \\\n",
    "enter your data specific adaptor sequence. Please selec appropriate option.')\n",
    "print('1. Default adaptor sequence: TCGTATGCCGTCTTCTGCTTG')\n",
    "print('2. User specific adaptor sequence')\n",
    "user_choice =int(input('Please select your option:'))\n",
    "\n",
    "if user_choice == 1:\n",
    "    adaptor = 'TGGAATTCTCGGGTGCCAAGG'\n",
    "    os.environ['adaptor'] = adaptor\n",
    "elif user_choice == 2:\n",
    "    adaptor = input('Please enter the adaptor sequence')\n",
    "    os.environ['adaptor'] = adaptor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add script for adaptor trimming here using trim_galore\n",
    "!bash scripts/adaptor_trimming.sh\n",
    "\n",
    "# Add script for read length based spliting (into 3 parts) here using bbduk\n",
    "!bash scripts/fastq_split.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading the Mirdeep* aligner and mirbase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Please enter your choice: ')\n",
    "print(\"Option 1: hg19 based alignment using Mirdeep* with miRBase v19 \")\n",
    "print(\"Option 2: hg19 based alignment using Mirdeep* with miRBase v20\")\n",
    "print(\"Option 3: hg38 based alignment using Mirdeep* with miRBase 21\")\n",
    "print(\"Option 4: hg38 based alignment using Mirdeep* with miRBase 22 (Default Condition)\")\n",
    "print(\"Please enter either 1, 2, 3 or 4\")\n",
    "choice = int(input(\"Please enter your choice:  \"))\n",
    "\n",
    "if choice == 1:\n",
    "    print(\"You chose hg19 and miRBase v19 based alignment using Mirdeep*\")\n",
    "    with open('data/mirdeep.conf',\"w\") as handle:\n",
    "        handle.write(\"Human Genome = hg19\\nmiRBase = 19\")   \n",
    "    print(\"Downloading Mirdeep*....\")\n",
    "    command = \"wget --content-disposition  http://sourceforge.net/projects/mirdeepstar/files/MDS_command_line_v37.zip -O $Tools_DIR/MDS_command_line_v37.zip && \"\n",
    "    command += \"unzip -o $Tools_DIR/MDS_command_line_v37.zip -d $Tools_DIR\"\n",
    "    subprocess.call(command, shell=True)    \n",
    "    print(\"Downloading Mirdeep* is complete. Now downloading hg19 human reference genome....\")\n",
    "    ref_var = 'hg19'    \n",
    "    os.environ['REF_DIR'] = os.path.join(os.environ['HOME_DIR'], 'Tools',\n",
    "                                     'MDS_command_line_v37/MDS_command_line')\n",
    "    print(\"\")\n",
    "    command = \"\"\n",
    "    command += \"mkdir $HOME_DIR/refs/hg19 && \"\n",
    "    command += \"wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz -P $HOME_DIR/refs/hg19/ && \"\n",
    "    command += \"pigz -p 5 -d $HOME_DIR/refs/hg19/hg19.fa.gz && echo Download complete. Now building bowtie indexes && \"\n",
    "    command += \"$HOME_DIR/Tools/bowtie-1.2.3-linux-x86_64/bowtie-build --threads 8 $HOME_DIR/refs/hg19/hg19.fa $HOME_DIR/refs/hg19/hg19\"\n",
    "    subprocess.call(command, shell=True)  \n",
    "    print(\"Human refenrence genome hg19 has been downloaded and index files generation complete.\")\n",
    "    \n",
    "elif choice == 2:\n",
    "    print(\"You chose hg19 and miRBase v20 based alignment using Mirdeep*\")\n",
    "    with open('data/mirdeep.conf',\"w\") as handle:\n",
    "        handle.write(\"Human Genome = hg19\\nmiRBase = 20\")   \n",
    "    command = \"\"\n",
    "    command = \"wget --content-disposition  http://sourceforge.net/projects/mirdeepstar/files/MDS_command_line_v37.zip -O $Tools_DIR/MDS_command_line_v37.zip && \"\n",
    "    command += \"unzip -o $Tools_DIR/MDS_command_line_v37.zip -d $Tools_DIR && \"\n",
    "    command += \"rm $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/* && \"\n",
    "    command += \"wget --content-disposition https://www.mirbase.org/ftp/20/hairpin.fa.gz -P $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase && \"\n",
    "    command += \"pigz -p 5 -d $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/hairpin.fa.gz && \"\n",
    "    command += \"wget --content-disposition  https://www.mirbase.org/ftp/20/mature.fa.gz -P $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase && \"\n",
    "    command += \"pigz -p 5 -d $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/mature.fa.gz &&  \"\n",
    "    command += \"wget --content-disposition  https://www.mirbase.org/ftp/20/genomes/hsa.gff3 -P $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase && \"\n",
    "    command += \"mv $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/hsa.gff3 $HOME_DIR/Tools/MDS_command_line_v37/MDS_command_line/genome/hg19/miRBase/knownMiR.gff3\"                \n",
    "    subprocess.call(command, shell=True)   \n",
    "    print(\"Downloading Mirdeep* is complete. Now downloading hg19 human reference genome....\")\n",
    "    ref_var = 'hg19'    \n",
    "    os.environ['REF_DIR'] = os.path.join(os.environ['HOME_DIR'], 'Tools',\n",
    "                                     'MDS_command_line_v37/MDS_command_line')\n",
    "    print(\"\")\n",
    "    command = \"\"\n",
    "    command += \"mkdir $HOME_DIR/refs/hg19 && \"\n",
    "    command += \"wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz -P $HOME_DIR/refs/hg19 && \"\n",
    "    command += \"pigz -p 5 -d $HOME_DIR/refs/hg19/hg19.fa.gz && echo Download complete. Now building bowtie indexes && \"\n",
    "    command += \"$HOME_DIR/Tools/bowtie-1.2.3-linux-x86_64/bowtie-build --threads 8 $HOME_DIR/refs/hg19/hg19.fa $HOME_DIR/refs/hg19/hg19\"\n",
    "    subprocess.call(command, shell=True)    \n",
    "    print(\"Human refenrence genome hg19 has been downloaded and index files generation complete.\")\n",
    "\n",
    "elif choice == 3:\n",
    "    print(\"You chose hg38 and miRBase v21 based genome alignment using Mirdeep*\")\n",
    "    with open('data/mirdeep.conf',\"w\") as handle:\n",
    "        handle.write(\"Human Genome = hg38\\nmiRBase = 21\")   \n",
    "    print(\"Downloading Mirdeep*....\")\n",
    "    command = \"\"\n",
    "    command += \"rm -rf $Tools_DIR/MDS_command_line_v38/MDS_command_line/genome/hg38 && \" \n",
    "    command += \"wget --content-disposition  http://sourceforge.net/projects/mirdeepstar/files/Index_files/hg38.zip/download -P $Tools_DIR/ && \"\n",
    "    command += \"unzip -o $Tools_DIR/hg38.zip -d $Tools_DIR/MDS_command_line_v38/MDS_command_line/genome/ && \"\n",
    "    command += \"rm -r $Tools_DIR/MDS_command_line_v38/MDS_command_line/genome/hg19\"\n",
    "    subprocess.call(command, shell=True)   \n",
    "    print(\"Downloading Mirdeep* is complete.\")\n",
    "    ref_var = 'hg38'   \n",
    "\n",
    "elif choice == 4:\n",
    "    print(\"You chose hg38 and miRBase v22 based genome alignment using Mirdeep*(Default Condition)\")\n",
    "    with open('data/mirdeep.conf',\"w\") as handle:\n",
    "        handle.write(\"Human Genome = hg38\\nmiRBase = 22\")   \n",
    "    ref_var = 'hg38'\n",
    "\n",
    "else:\n",
    "    print(\"Please enter valid option\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## piRNA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piRNA_pipeline():\n",
    "    cmd = 'bash scripts/piRNA_pipeline.sh'\n",
    "    os.system(cmd)\n",
    "\n",
    "threading.Thread(target=piRNA_pipeline).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## miRNA sequence Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the synthetic data experiments are performed only on 1 sample that contains reads generated by seed-based simulator miRSim. We don't need multiple threads functionality of miRPipe. So We have uncommented the simple sequential sample processing bash script for sequence alignment using miRDeep* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequential sample processing\n",
    "!bash $HOME_DIR/scripts/Mirdeep_star.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing batches for miRNA sequence alignment for multi-thread processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isTreadAlive():\n",
    "#     for t in threads:\n",
    "#         if t.isAlive():\n",
    "#             return 1\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mirdeep_star(batch,batch_id):    \n",
    "#     os.chdir(os.environ['REF_DIR'])\n",
    "#     files = os.listdir(os.getcwd())    \n",
    "#     ref_id = os.path.join(os.getcwd(),'genome')\n",
    "#     if 'hg19' in os.listdir(ref_id):\n",
    "#         ref_idx = 'hg19'\n",
    "#     elif 'hg38' in os.listdir(ref_id):\n",
    "#         ref_idx = 'hg38'\n",
    "#     else:\n",
    "#         print('Reference index are missing in ', ref_id)\n",
    "#         sys.exit(2)\n",
    "    \n",
    "#     batch = pd.read_csv(os.path.join(os.environ['SEQ_DIR'],batch),index_col=0)\n",
    "#     for idx in range(batch.shape[0]):\n",
    "#         basename = batch.iloc[idx,0].split('.')[0] +'_trimmed.fastq'\n",
    "#         path_new = os.path.join(os.environ['SEQ_DIR'],'fastq_17_24',basename)\n",
    "        \n",
    "#         # Constraining miRDeep* to take only 4 gb in each thread\n",
    "#         try:\n",
    "#             command = 'java -Xmx4096m -jar MD.jar -g '+ ref_idx + ' -a ' + os.environ['adaptor'] + ' -t 17 -l 24 -s -20 -r 5 -p 20 -m 101 ' + path_new + ' && rm ' + path_new\n",
    "#             os.system(command)\n",
    "#         except:\n",
    "#             screenlock.acquire()\n",
    "#             print('Sample ',basename,' is not present in the directory.')\n",
    "#             screenlock.release()\n",
    "    \n",
    "#     screenlock.acquire()\n",
    "#     print('---------------------------')\n",
    "#     print(batch_id, ' is complete.')\n",
    "#     print('---------------------------')\n",
    "#     screenlock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the sample_list provided by user\n",
    "# os.chdir(os.environ['HOME_DIR'])\n",
    "# sample_list = pd.read_csv('data/sample_list.csv',index_col=0)\n",
    "\n",
    "# # Generating batches\n",
    "# if sample_list.shape[0] < 10:\n",
    "#     no_of_batch = 4\n",
    "# elif sample_list.shape[0] > 10 and sample_list.shape[0] <= 30:\n",
    "#     no_of_batch = 8\n",
    "# elif sample_list.shape[0] > 30 and sample_list.shape[0] <= 50:\n",
    "#     no_of_batch = 12\n",
    "# elif sample_list.shape[0] > 50 and sample_list.shape[0] <= 100:\n",
    "#     no_of_batch = 16\n",
    "# elif sample_list.shape[0] > 100:\n",
    "#     no_of_batch = 20\n",
    "    \n",
    "# file_list_splitted = np.array_split(sample_list, no_of_batch)\n",
    "# for batch_idx in range(len(file_list_splitted)):\n",
    "#     file_list_splitted[batch_idx].to_csv('data/batch_'+str(batch_idx+1)+'.csv',encoding='utf-8',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling mirdeep* for each batch in individual threads.\n",
    "# threads = []\n",
    "# for i in range(1,no_of_batch+1):\n",
    "#     print('Running batch_'+ str(i)+ ' on thread-'+ str(i))\n",
    "#     t = threading.Thread(target=mirdeep_star,args=('batch_'+str(i)+'.csv', 'batch_'+str(i),))\n",
    "#     threads.append(t)\n",
    "#     t.start()\n",
    "\n",
    "# flag =1\n",
    "# while (flag):\n",
    "#     time.sleep(5.0)\n",
    "#     flag = isTreadAlive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Alignment Analysis\n",
    "## **Processing of raw counts from Mirdeep* results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.getcwd())\n",
    "files = [] \n",
    "[files.append(i) for i in os.listdir(\"data/fastq_17_24\") if (\".result\" in i and not \"known\" in i)]\n",
    "result_data = {}\n",
    "print('Preparing master look up table for result data')\n",
    "for file in tqdm(files):    \n",
    "    result_data['_'.join(file.split(\"_\")[:-1])] = {}\n",
    "    readfile = open(os.path.join(\"data/fastq_17_24\", file), 'r').readlines()\n",
    "    header = readfile[0].split(\"\\t\")\n",
    "    for line in readfile[1:]:\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]] = {}\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[1]] = line.split(\"\\t\")[1]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[2]] = line.split(\"\\t\")[2]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[3]] = line.split(\"\\t\")[3]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[4]] = line.split(\"\\t\")[4]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[5]] = int(line.split(\"\\t\")[5])\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[6]] = line.split(\"\\t\")[6]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[7]] = line.split(\"\\t\")[7]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[8]] = line.split(\"\\t\")[8]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[9]] = line.split(\"\\t\")[9]\n",
    "        result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[10]] = line.split(\"\\t\")[10]\n",
    "\n",
    "    known_miR_filename = file.split('.')[0] + '.known_miR.result'\n",
    "    readfile = open(os.path.join(\"data/fastq_17_24\", known_miR_filename), 'r').readlines()\n",
    "    for line in readfile[1:]:\n",
    "        if not line.split(\"\\t\")[0] in result_data['_'.join(file.split(\"_\")[:-1])].keys():\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]] = {}\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[1]] = line.split(\"\\t\")[1]\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[2]] = line.split(\"\\t\")[2]\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[3]] = line.split(\"\\t\")[3]\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[4]] = line.split(\"\\t\")[4]\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[5]] = int(line.split(\"\\t\")[5])\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[6]] = line.split(\"\\t\")[6]\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[7]] = line.split(\"\\t\")[7]\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[8]] = line.split(\"\\t\")[8]\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[9]] = line.split(\"\\t\")[9].split('(')[0].lower()\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[10]] = line.split(\"\\t\")[10]\n",
    "        else:\n",
    "            result_data['_'.join(file.split(\"_\")[:-1])][line.split(\"\\t\")[0]][header[5]] += int(line.split(\"\\t\")[5])\n",
    "\n",
    "\n",
    "        \n",
    "print('Master lookup table is generated. Now collecting all unique miRs from master lookup table')\n",
    "mir_dict = {}\n",
    "for file in tqdm(result_data.keys()):\n",
    "    # print('Working on ',file)\n",
    "    for k,v in result_data[file].items():\n",
    "        i = 1\n",
    "        if not k in mir_dict.keys():\n",
    "            mir_dict[k] = [v['chr'],v['mature_loci'].split('-')[0],\n",
    "                           v['mature_loci'].split('-')[1],v['mature miR'],v['sequence']]\n",
    "        else:\n",
    "            mir_dict1 = mir_dict.copy()\n",
    "            for k1,v1 in mir_dict1.items():\n",
    "                new_loc_start = int(v['mature_loci'].split('-')[0])\n",
    "                new_loc_end = int(v['mature_loci'].split('-')[0])\n",
    "                new_mir_seq = v['mature miR']\n",
    "                if k1 == k:                    \n",
    "                    if v1[0] == v['chr']:                          \n",
    "                        if (new_loc_start == int(v1[1])) and (new_loc_end == int(v1[2])):\n",
    "                            if new_mir_seq == v1[3]:\n",
    "                                pass\n",
    "                    else:\n",
    "                        mir_dict[k+'_'+str(i)] = [v['chr'],v['mature_loci'].split('-')[0],\n",
    "                                       v['mature_loci'].split('-')[1],v['mature miR'],v['sequence']]\n",
    "                        i += 1\n",
    "\n",
    "\n",
    "print('Prepating dataframe for all unique miRs and samples....')\n",
    "counts = pd.DataFrame(columns=list(result_data.keys()))\n",
    "mir_name = list(mir_dict.keys())\n",
    "index = 0\n",
    "\n",
    "for k,v in tqdm(mir_dict.items()):\n",
    "    for file in result_data.keys():\n",
    "        for k1,v1 in result_data[file].items():\n",
    "            if v1['mature miR'] == v[3]:\n",
    "                loc_start = int(v1['mature_loci'].split('-')[0])\n",
    "                loc_end = int(v1['mature_loci'].split('-')[1])\n",
    "                loc_chr = v1['chr']\n",
    "                if loc_chr == v[0]:\n",
    "                    if loc_start == int(v[1]) and loc_end == int(v[2]):   \n",
    "                        counts.loc[k,file] = int(v1['expression(number of mature reads)'])\n",
    "                   \n",
    "counts['mir_ID'] = mir_name\n",
    "counts = counts.set_index('mir_ID')\n",
    "counts['chr'] = [v[0] for v in mir_dict.values()]\n",
    "counts['chr_start'] = [v[1] for v in mir_dict.values()]\n",
    "counts['chr_end'] = [v[2] for v in mir_dict.values()]\n",
    "counts['miRNA_sequence'] = [v[3] for v in mir_dict.values()]\n",
    "counts['miRNA_precursor_sequence'] = [v[4].lower() for v in mir_dict.values()]\n",
    "# rearranging columns\n",
    "cols = counts.columns.tolist()\n",
    "cols = cols[-5:]+ cols[:-5]\n",
    "counts = counts[cols]\n",
    "\n",
    "counts = counts.fillna(0)\n",
    "print(counts.shape)\n",
    "counts.to_csv(\"data/counts.csv\",encoding='utf-8',index=True)\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_known = counts[counts.index.str.contains('hsa')]\n",
    "counts_unknown = counts[~counts.index.str.contains('hsa')]\n",
    "print('All %d known and %d novel miRNA are successfully separated' %(counts_known.shape[0],counts_unknown.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blast Search\n",
    "## Search for Novel miRNA sequence in DashR Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Run this cell only when running this notebook first time. After successfully running this, \n",
    "   pickle file will be saved which can be used further without generating the \n",
    "   results again and will save lot of time.\n",
    "   This code has been tested with Firefox version 66.0.4 (64-bit).\n",
    "'''\n",
    "\n",
    "novel_mirs = list(counts_unknown.index.values)\n",
    "gecko = os.path.abspath('geckodriver')\n",
    "dashR_Results = []\n",
    "browser = Browser('firefox',executable_path=gecko,headless=True)\n",
    "browser.visit('http://dashr2.lisanwanglab.org/search.php#')\n",
    "\n",
    "if ref_var == 'hg19':\n",
    "    #  Select DASHR2 GEO HG19 as reference\n",
    "    xpath = '/html/body/div/div[1]/div/div/select/option[3]' \n",
    "else:\n",
    "    #  Select DASHR2 GEO HG38 as reference\n",
    "    xpath = '/html/body/div/div[1]/div/div/select/option[4]'\n",
    "browser.find_by_xpath(xpath).click()\n",
    "time.sleep(3)\n",
    "\n",
    "browser.find_by_text('Search by sequence ').click()\n",
    "time.sleep(3)\n",
    "n_idx = len(list(counts_unknown.loc[:,'miRNA_sequence']))\n",
    "n = 0\n",
    "for idx in tqdm(range(n_idx)):\n",
    "    name = list(counts_unknown.index.values)[idx]\n",
    "    seq = list(counts_unknown.loc[:,'miRNA_sequence'])[idx]\n",
    "    dashR_Results.append(name)\n",
    "    browser.fill('querySeq', seq)\n",
    "    time.sleep(2)\n",
    "    xpath = '//*[@id=\"search\"]/div[4]/div/button'    \n",
    "    browser.find_by_xpath(xpath).click()\n",
    "    if not browser.is_text_present('No matches found'):\n",
    "        n += 1\n",
    "        xpath = '//*[@id=\"sequence-results\"]/pre/table' \n",
    "        results = browser.find_by_xpath(xpath)\n",
    "        i=0\n",
    "        for search_result in results:\n",
    "            title = search_result.text.encode('utf8') \n",
    "            link = search_result[\"href\"]             \n",
    "            dashR_Results.append((title, link)) \n",
    "            i += 1   \n",
    "    else:\n",
    "        seq = str(Seq(seq).reverse_complement())\n",
    "        browser.fill('querySeq', seq)\n",
    "        time.sleep(2)\n",
    "        xpath = '//*[@id=\"search\"]/div[4]/div/button'    \n",
    "        browser.find_by_xpath(xpath).click()\n",
    "        if not browser.is_text_present('No matches found'):\n",
    "            n += 1\n",
    "            xpath = '//*[@id=\"sequence-results\"]/pre/table' \n",
    "            results = browser.find_by_xpath(xpath)\n",
    "            i=0\n",
    "            for search_result in results:\n",
    "                title = search_result.text.encode('utf8') \n",
    "                link = search_result[\"href\"]             \n",
    "                dashR_Results.append((title, link)) \n",
    "                i += 1\n",
    "        else:\n",
    "            dashR_Results.append('miRNA NOT FOUND')\n",
    "    \n",
    "\n",
    "\n",
    "browser.quit()\n",
    "with open(\"data/dashR_Results.pickle\", 'wb') as handle:\n",
    "    pickle.dump(dashR_Results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell if you want to load the pickle file obtained from dashR search module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#    Get DashR database searching results from saved pickle file.   \n",
    "# '''\n",
    "# import pickle\n",
    "# print('Loading saved results from DashR Database....')\n",
    "# with open(\"data/dashR_Results.pickle\", 'rb') as handle:\n",
    "#     dashR_Results = pickle.load(handle)\n",
    "# len(dashR_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DashR Results post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_check(rna_name, df1,idx):\n",
    "    \n",
    "    rna_name1 = rna_name.split(' ')[1]\n",
    "    if '3p' in rna_name1 or '5p' in rna_name1:\n",
    "        rna_name1 = rna_name1[:-3]\n",
    "    rna_name1 = rna_name1.lower()\n",
    "    \n",
    "    rna_chr1 = counts_unknown.loc[dashR_Results[idx],'chr']\n",
    "    rna_chr_start1 = int(counts_unknown.loc[dashR_Results[idx],'chr_start'])\n",
    "    rna_chr_end1 = int(counts_unknown.loc[dashR_Results[idx],'chr_end'])\n",
    "\n",
    "    if not counts_unknown.shape[0] == df1.shape[0]:\n",
    "        try:\n",
    "            rna_chr2 = df1.loc[rna_name1,'chr']\n",
    "            rna_chr_start2 = int(df1.loc[rna_name1,'chr_start'])\n",
    "            rna_chr_end2 = int(df1.loc[rna_name1,'chr_end'])\n",
    "        except:\n",
    "            rna_chr2 = df1.loc[dashR_Results[idx],'chr']\n",
    "            rna_chr_start2 = int(df1.loc[dashR_Results[idx],'chr_start'])\n",
    "            rna_chr_end2 = int(df1.loc[dashR_Results[idx],'chr_end'])\n",
    "\n",
    "    else:\n",
    "        rna_chr2 = rna_name.split(' ')[3].split(':')[0]\n",
    "        rna_chr_start2 = int(rna_name.split(' ')[3].split(':')[1].split('[')[0].split('-')[0])\n",
    "        rna_chr_end2 = int(rna_name.split(' ')[3].split(':')[1].split('[')[0].split('-')[1])\n",
    "    \n",
    "    if (rna_chr1 == rna_chr2) and np.abs(rna_chr_start1 - rna_chr_start2)<=2 and np.abs(rna_chr_end1 - rna_chr_end2)<=2:\n",
    "        flag = True\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_unknown2 = counts_unknown.copy()\n",
    "counts_known2 = counts_known.copy()\n",
    "counts_unknown2 = counts_unknown2.drop(['miRNA_sequence','chr','chr_start','chr_end','miRNA_precursor_sequence'],axis=1)\n",
    "counts_known2 = counts_known2.drop(['miRNA_sequence','chr','chr_start','chr_end','miRNA_precursor_sequence'],axis=1)\n",
    "novel_mir_old_index = counts_unknown.index.tolist()\n",
    "paralogue_dict = {}\n",
    "\n",
    "for idx in (range(0,len(dashR_Results),2)):\n",
    "    results = dashR_Results[idx+1]    \n",
    "    if isinstance((dashR_Results[idx+1]),tuple):\n",
    "        results = list(dashR_Results[idx+1])[0].decode(\"utf-8\")        \n",
    "        rna_type_results = results.split('\\n')[-1]\n",
    "        rna_type1 = rna_type_results.split(' ')[1].lower()\n",
    "        if 'hsa-mir' in rna_type1:                            \n",
    "            idx2 = list(counts_unknown2.index.values).index(dashR_Results[idx])\n",
    "            count_to_add = list(map(int,list(counts_unknown2.iloc[idx2,:].values)))\n",
    "            if '3p' in rna_type1 or '5p' in rna_type1:\n",
    "                rna_type1 = rna_type1[:-3]\n",
    "            if rna_type1.lower() in list(counts_known.index.values):    \n",
    "                counts_add_flag = loc_check(rna_type_results,counts_known,idx)\n",
    "                if counts_add_flag:                        \n",
    "                    count_increament = list(counts_known2.loc[rna_type1.lower(),:].values)                        \n",
    "                    count_increament = list(map(add,count_increament,count_to_add))               \n",
    "                    counts_known2.iloc[list(counts_known2.index.values).index(rna_type1.lower()),:] = count_increament\n",
    "                    counts_unknown2 = counts_unknown2.drop([str(dashR_Results[idx])],axis = 0)                    \n",
    "                    novel_mir_old_index.remove(novel_mir_old_index[novel_mir_old_index.index(dashR_Results[idx])])\n",
    "                else:                    \n",
    "                    if rna_type1 in paralogue_dict.keys():\n",
    "                        char_string = paralogue_dict[rna_type1][-1].split('_')[-1]\n",
    "                        char_no = int(char_string) + 1\n",
    "                    else:\n",
    "                        char_no = 1\n",
    "\n",
    "                    new_name = rna_type1 + '_' + str(char_no)\n",
    "                    unique_flag = False\n",
    "                    while unique_flag == False:\n",
    "                        if new_name in counts_known2.index.tolist():\n",
    "                            new_name = new_name[:-2]+'_' + str(int(new_name[-1])+1)\n",
    "                        else:\n",
    "                            unique_flag = True\n",
    "\n",
    "                    old_index = counts_unknown2.index.tolist()                        \n",
    "                    old_index[idx2] = new_name\n",
    "                    counts_unknown2.index = old_index\n",
    "                    if rna_type1 in paralogue_dict.keys():\n",
    "                        paralogue_dict[rna_type1].append(new_name)\n",
    "                    else:\n",
    "                        paralogue_dict[rna_type1] = []\n",
    "                        paralogue_dict[rna_type1].append(new_name)\n",
    "            else:\n",
    "                mir_add_flag = loc_check(rna_type_results,counts_unknown,idx)\n",
    "                if mir_add_flag:\n",
    "                    char_no += 1\n",
    "                    # Novel mir name will be changed to known mir name\n",
    "                    if old_index[idx2] in counts_known2.index.tolist():\n",
    "                        old_index[idx2] = old_index[idx2][:-2] + '_'+ str(char_no)\n",
    "                        char_no += 1\n",
    "                    old_index = counts_unknown2.index.tolist()\n",
    "                    old_index[idx2] = rna_type1[:-2] + '_'+ str(char_no)\n",
    "                    counts_unknown2.index = old_index\n",
    "                else:\n",
    "                    # Novel miRNA is the paralogues of known mirna\n",
    "                    if rna_type1 in paralogue_dict.keys():\n",
    "                        char_string = paralogue_dict[rna_type1][-1].split('_')[-1]\n",
    "                        char_no = int(char_string) + 1\n",
    "                    else:\n",
    "                        char_no = 1\n",
    "\n",
    "                    new_name = rna_type1 + '_' + str(char_no)\n",
    "                    unique_flag = False\n",
    "                    while unique_flag == False:\n",
    "                        if new_name in counts_known2.index.tolist():\n",
    "                            new_name = new_name[:-2]+'_' + str(int(new_name[-1])+1)\n",
    "                        else:\n",
    "                            unique_flag = True\n",
    "                    old_index = counts_unknown2.index.tolist()                        \n",
    "                    old_index[idx2] = new_name\n",
    "                    counts_unknown2.index = old_index \n",
    "                    if rna_type1 in paralogue_dict.keys():\n",
    "                        paralogue_dict[rna_type1].append(new_name)\n",
    "                    else:\n",
    "                        paralogue_dict[rna_type1] = []\n",
    "                        paralogue_dict[rna_type1].append(new_name)\n",
    "            \n",
    "\n",
    "counts_known2['chr'] = counts_known[[\"chr\"]]\n",
    "counts_known2['chr_start'] = counts_known[[\"chr_start\"]]\n",
    "counts_known2['chr_end'] = counts_known[[\"chr_end\"]]\n",
    "counts_known2['miRNA_sequence'] = counts_known[[\"miRNA_sequence\"]]\n",
    "counts_known2['miRNA_precursor_sequence'] = counts_known[[\"miRNA_precursor_sequence\"]]\n",
    "\n",
    "new_mir_list = counts_unknown2.index.tolist()\n",
    "counts_unknown2.index = novel_mir_old_index\n",
    "counts_unknown2['chr'] = counts_unknown[[\"chr\"]]\n",
    "counts_unknown2['chr_start'] = counts_unknown[[\"chr_start\"]]\n",
    "counts_unknown2['chr_end'] = counts_unknown[[\"chr_end\"]]\n",
    "counts_unknown2['miRNA_sequence'] = counts_unknown[[\"miRNA_sequence\"]]\n",
    "counts_unknown2['miRNA_precursor_sequence'] = counts_unknown[[\"miRNA_precursor_sequence\"]]\n",
    "counts_unknown2.index = new_mir_list\n",
    "\n",
    "# rearranging columns\n",
    "cols = counts_known2.columns.tolist()\n",
    "cols = cols[-5:]+ cols[:-5]\n",
    "counts_known2 = counts_known2[cols]\n",
    "\n",
    "cols = counts_unknown2.columns.tolist()\n",
    "cols = cols[-5:]+ cols[:-5]\n",
    "counts_unknown2 = counts_unknown2[cols]\n",
    "\n",
    "# add 2 dfs\n",
    "new_mirs_df = counts_unknown2[counts_unknown2.index.str.contains('hsa')]\n",
    "frames = [counts_known2, new_mirs_df]\n",
    "counts_known2 = pd.concat(frames)\n",
    "counts_unknown2 = counts_unknown2[~counts_unknown2.index.str.contains('hsa')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_unknown2.shape)\n",
    "print(counts_known2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed based Sequence Clustering\n",
    "## **Dictionary Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_known2 = counts_known2.drop_duplicates(subset=['miRNA_sequence'], keep='first')\n",
    "seed_dict = {}\n",
    "Xseed_dict = {}\n",
    "unknown_mir_dict = {}\n",
    "for index in counts_unknown2.index:\n",
    "    seed = counts_unknown2.loc[index,'miRNA_sequence'][1:7]\n",
    "    Xseed = counts_unknown2.loc[index,'miRNA_sequence'][7:]\n",
    "    unknown_mir_dict[index] = [seed,counts_unknown2.loc[index,'chr'],counts_unknown2.loc[index,'chr_start'],\n",
    "                              counts_unknown2.loc[index,'chr_end'],counts_unknown2.loc[index,'miRNA_sequence']]\n",
    "    seed_dict[index] = seed\n",
    "    Xseed_dict[index] = Xseed\n",
    "\n",
    "for index in counts_known2.index:\n",
    "    seed = counts_known2.loc[index,'miRNA_sequence'][1:7]\n",
    "    Xseed = counts_known2.loc[index,'miRNA_sequence'][7:]\n",
    "    unknown_mir_dict[index] = [seed,counts_known2.loc[index,'chr'],counts_known2.loc[index,'chr_start'],\n",
    "                              counts_known2.loc[index,'chr_end'],counts_known2.loc[index,'miRNA_sequence']]\n",
    "    seed_dict[index] = seed\n",
    "    Xseed_dict[index] = Xseed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "exact_seed_cluster = defaultdict(list) \n",
    "for key,values in seed_dict.items():\n",
    "    exact_seed_cluster[values].append(key)\n",
    "    \n",
    "exact_seed_cluster = dict(exact_seed_cluster)\n",
    "\n",
    "exact_Xseed_cluster = defaultdict(list) \n",
    "for key,values in Xseed_dict.items():\n",
    "    exact_Xseed_cluster[values].append(key)\n",
    "    \n",
    "exact_Xseed_cluster = dict(exact_Xseed_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fasta File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "writer = open(\"data/seed_seqs.fasta\", 'w')\n",
    "for key in unknown_mir_dict.keys():\n",
    "    line = \">\" + str(count) + \"\\t\" + key + \"\\n\"\n",
    "    line += str(unknown_mir_dict[key][0]) + \"\\n\"\n",
    "    writer.write(line)\n",
    "    count+=1\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "writer = open(\"data/seed_outer_seqs.fasta\", 'w')\n",
    "for key in unknown_mir_dict.keys():\n",
    "    line = \">\" + str(count) + \"\\t\" + key + \"\\n\"\n",
    "    line += str(unknown_mir_dict[key][-1][7:]) + \"\\n\"\n",
    "    writer.write(line)\n",
    "    count+=1\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD-HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute CD-HIT command\n",
    "\n",
    "!cd Tools/cd-hit-v4.6.8-2017-1208/; echo \"***** Clustering miRs Seed Sequences Only**********\"; \\\n",
    "                                                           ./cd-hit -l 5 -n 2 -c 1 \\\n",
    "                                                                    -i ../../data/seed_seqs.fasta \\\n",
    "                                                                    -o ../../data/cdhit_seed;     \\\n",
    "                                                           echo \" \"; \\\n",
    "                                                           echo \"***** Clustering miRs Rest Sequences Only**********\"; \\\n",
    "                                                           ./cd-hit -l 8 -i ../../data/seed_outer_seqs.fasta \\\n",
    "                                                                    -o ../../data/cdhit_seed_outer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing lookup table for Xseed Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CLuster dictionary preparation\n",
    "Cluster : cluster_no:[mir1,mir2]\n",
    "\"\"\"\n",
    "def fetch_id(xseed_id_fasta, xid):\n",
    "    for line in xseed_id_fasta:\n",
    "        if line:\n",
    "            if '>' in line[0]:   \n",
    "                if xid == line.split('\\t')[0].split('>')[1]:\n",
    "                    return line.split('\\t')[1]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cluster_dict_preparation(cluster_file, fasta_file):\n",
    "    cluster_file = open(cluster_file,'r').read().split('\\n')\n",
    "    fasta_id = open(fasta_file,'r').read().split('\\n')\n",
    "    cluster_dict = {}    \n",
    "    for line in cluster_file:\n",
    "        if line:\n",
    "            if '>' in line[0]:\n",
    "                line_no = cluster_file.index(line)\n",
    "                flag = True    \n",
    "                k = line[1:].replace(' ','_')\n",
    "                xid = cluster_file[line_no+1].split('>')[1].split('.')[0]\n",
    "                cluster_dict[k] = [fetch_id(fasta_id,xid)]\n",
    "                i = 2\n",
    "                while flag:\n",
    "                    if cluster_file[line_no+i]:\n",
    "                        if '>' in cluster_file[line_no+i][0]:\n",
    "                            flag = False\n",
    "                            pass\n",
    "                        else:\n",
    "                            xid = cluster_file[line_no+i].split('>')[1].split('.')[0]\n",
    "                            cluster_dict[k].append(fetch_id(fasta_id,xid))\n",
    "                            i += 1\n",
    "                    else:\n",
    "                        flag = False\n",
    "                        \n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "seed_cluster = cluster_dict_preparation('data/cdhit_seed.clstr','data/seed_seqs.fasta')\n",
    "Xseed_cluster = cluster_dict_preparation('data/cdhit_seed_outer.clstr','data/seed_outer_seqs.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Only for outer seed region clusters\n",
    "rev_dict : mir_name:[cluster1,cluster2]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rev_dictionary(cluster):\n",
    "    rev_dict = defaultdict(list)\n",
    "    mir_list = []\n",
    "    for k in list(unknown_mir_dict.keys()):\n",
    "        mir_list.append(k)\n",
    "        \n",
    "    for mir in mir_list:\n",
    "        for k,v in cluster.items():\n",
    "            for mir1 in v:\n",
    "                if mir == mir1:\n",
    "                    rev_dict[mir].append(k)\n",
    "                    \n",
    "    return dict(rev_dict)\n",
    "\n",
    "\n",
    "exact_seed_rev_dict = rev_dictionary(exact_seed_cluster)\n",
    "seed_rev_dict = rev_dictionary(seed_cluster)\n",
    "Xseed_rev_dict = rev_dictionary(Xseed_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to update and merge the counts if all conditions (seed,seq and loc) are satisfied\n",
    "Input : df1,df2,mir1 (from df1), mir2 (from df2)\n",
    "Output : df1_modified (with updated counts of mir1) and df2_modified (with mir2 removed)\n",
    "\"\"\"\n",
    "\n",
    "def update_counts(df1,df2,mir1,mir2):\n",
    "    mir1_counts = list(df1.loc[mir1,:].values)[5:]\n",
    "    mir2_counts = list(df2.loc[mir2,:].values)[5:]\n",
    "    mir2_counts = list(map(add,mir1_counts,mir2_counts))\n",
    "    df1.loc[mir2,:] = list(df1.loc[mir2,:].values)[:5] + mir2_counts\n",
    "    df2 = df2.drop(mir1,axis = 0)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "novel_miRNA2 = counts_unknown2.copy()\n",
    "mir_reannotation = pd.DataFrame()\n",
    "mir_old_name = []\n",
    "mir_new_name = []\n",
    "novel_mir_seq = []\n",
    "family_name = []\n",
    "attribute = []\n",
    "paralogue_dict = {}\n",
    "\n",
    "for k,v in tqdm(seed_cluster.items()):    \n",
    "    char_no = 0\n",
    "    novel_mir_name = seed_cluster[k]\n",
    "    novel_mir_name = [i for i in novel_mir_name if not 'hsa' in i]\n",
    "    for n_mir in novel_mir_name:   \n",
    "        novel_mir_name = [i for i in novel_mir_name if i is not None]                        \n",
    "        if len(novel_mir_name)>1:  \n",
    "            for n_mir in novel_mir_name:\n",
    "                if not n_mir == None:\n",
    "                    char_no = 1\n",
    "                    novel_mir_name1 = novel_mir_name.copy()\n",
    "                    novel_mir_name1.remove(n_mir)\n",
    "                    n_chr_loc = unknown_mir_dict[n_mir][1]\n",
    "                    n_chr_loc_start = int(unknown_mir_dict[n_mir][2])\n",
    "                    n_chr_loc_end = int(unknown_mir_dict[n_mir][3])\n",
    "                    seed_common_mir = list(set(novel_mir_name1) & set(exact_seed_cluster[exact_seed_rev_dict[n_mir][0]]))\n",
    "                    if seed_common_mir:                    \n",
    "                        Xseed_common_mir = list(set(seed_common_mir) & set(Xseed_cluster[Xseed_rev_dict[n_mir][0]]))\n",
    "                        if Xseed_common_mir:\n",
    "                            for c_mir1 in Xseed_common_mir:\n",
    "                                c_chr_loc = unknown_mir_dict[c_mir1][1]\n",
    "                                c_chr_loc_start = int(unknown_mir_dict[c_mir1][2])\n",
    "                                c_chr_loc_end = int(unknown_mir_dict[c_mir1][3])\n",
    "                                if (n_chr_loc == c_chr_loc) and np.abs(n_chr_loc_start - c_chr_loc_start)<=2 and np.abs(n_chr_loc_end - c_chr_loc_end)<=2:\n",
    "                                    _,novel_miRNA2 = update_counts(novel_miRNA2,novel_miRNA2,c_mir1,n_mir)\n",
    "                                    novel_mir_name[novel_mir_name.index(c_mir1)] = None\n",
    "                                    mir_old_name.append(c_mir1)\n",
    "                                    mir_new_name.append(n_mir)\n",
    "                                    family_name.append(n_mir)\n",
    "                                    novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                    attribute.append('Same Seed, Xseed and loc')\n",
    "                                else:\n",
    "                                    if n_mir in paralogue_dict.keys():\n",
    "                                        char_string = paralogue_dict[n_mir][-1].split('_')[-1]\n",
    "                                        char_no = int(char_string) + 1\n",
    "                                    else:\n",
    "                                        char_no = 1\n",
    "                                        \n",
    "                                    new_name = n_mir+'_'+ str(char_no)\n",
    "                                    unique_flag = False\n",
    "                                    while unique_flag == False:\n",
    "                                        if new_name in novel_miRNA2.index.tolist():\n",
    "                                            new_name = new_name[:-2]+'_' + str(int(new_name[-1])+1)\n",
    "                                        else:\n",
    "                                            unique_flag = True\n",
    "                                    novel_miRNA2 = novel_miRNA2.rename(index={c_mir1 : new_name})\n",
    "                                    \"\"\"\n",
    "                                        The incoming miR shared same seed, altered xseed with different genomic location with cluster head.\n",
    "                                        We need to check whether the incoming miR shares the genomic location with the existing \n",
    "                                        members of the same cluster or not. If not, then it'll be added to the cluster with \n",
    "                                        different name. Otherwise, it'll be merged to the member with whom it shares the exact \n",
    "                                        seed, xseed and genomic location.\n",
    "                                    \"\"\"\n",
    "                                    \n",
    "                                    if n_mir in paralogue_dict.keys():\n",
    "                                        if len(paralogue_dict[n_mir]) >= 1:\n",
    "                                            for m in paralogue_dict[n_mir]:\n",
    "                                                if m in novel_miRNA2.index.tolist():\n",
    "                                                    if (novel_miRNA2.loc[m,'chr'] == c_chr_loc) and np.abs(int(novel_miRNA2.loc[m,'chr_start']) - int(novel_miRNA2.loc[new_name,'chr_start']))<=2 and np.abs(int(novel_miRNA2.loc[m,'chr_end']) - int(novel_miRNA2.loc[new_name,'chr_end']))<=2:\n",
    "                                                        _,novel_miRNA2 = update_counts(novel_miRNA2,novel_miRNA2,new_name,m)\n",
    "                                                        mir_old_name.append(c_mir1)\n",
    "                                                        mir_new_name.append(m)\n",
    "                                                        family_name.append(m)\n",
    "                                                        novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                                        attribute.append('Same Seed, Xseed and loc')\n",
    "                                                        break\n",
    "                                    novel_mir_name[novel_mir_name.index(c_mir1)] = None\n",
    "                                    mir_old_name.append(c_mir1)\n",
    "                                    mir_new_name.append(new_name)\n",
    "                                    family_name.append(n_mir)\n",
    "                                    novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                    attribute.append('Paralogues')                                    \n",
    "                                    if n_mir in paralogue_dict.keys():\n",
    "                                        paralogue_dict[n_mir].append(new_name)\n",
    "                                    else:\n",
    "                                        paralogue_dict[n_mir] = []\n",
    "                                        paralogue_dict[n_mir].append(new_name)\n",
    "                        else:\n",
    "                            for c_mir1 in seed_common_mir: \n",
    "                                c_chr_loc = unknown_mir_dict[c_mir1][1]\n",
    "                                c_chr_loc_start = int(unknown_mir_dict[c_mir1][2])\n",
    "                                c_chr_loc_end = int(unknown_mir_dict[c_mir1][3])                               \n",
    "                                if n_mir in paralogue_dict.keys():\n",
    "                                        char_string = paralogue_dict[n_mir][-1].split('_')[-1]\n",
    "                                        char_no = int(char_string) + 1\n",
    "                                else:\n",
    "                                    char_no = 1\n",
    "                                    \n",
    "                                new_name = n_mir+'_'+ str(char_no)\n",
    "                                unique_flag = False\n",
    "                                while unique_flag == False:\n",
    "                                    if new_name in novel_miRNA2.index.tolist():\n",
    "                                        new_name = new_name[:-2]+'_' + str(int(new_name[-1])+1)\n",
    "                                    else:\n",
    "                                        unique_flag = True\n",
    "                                novel_miRNA2 = novel_miRNA2.rename(index={c_mir1 : new_name})\n",
    "                                if n_mir in paralogue_dict.keys():\n",
    "                                        if len(paralogue_dict[n_mir]) >= 1:\n",
    "                                            for m in paralogue_dict[n_mir]:\n",
    "                                                if m in novel_miRNA2.index.tolist():\n",
    "                                                    if (novel_miRNA2.loc[m,'chr'] == c_chr_loc) and np.abs(int(novel_miRNA2.loc[m,'chr_start']) - int(novel_miRNA2.loc[new_name,'chr_start']))<=2 and np.abs(int(novel_miRNA2.loc[m,'chr_end']) - int(novel_miRNA2.loc[new_name,'chr_end']))<=2:\n",
    "                                                        _,novel_miRNA2 = update_counts(novel_miRNA2,novel_miRNA2,new_name,m)\n",
    "                                                        mir_old_name.append(c_mir1)\n",
    "                                                        mir_new_name.append(m)\n",
    "                                                        family_name.append(m)\n",
    "                                                        novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                                        attribute.append('Same Seed, Xseed and loc')\n",
    "                                                        break\n",
    "                                novel_mir_name[novel_mir_name.index(c_mir1)] = None\n",
    "                                mir_old_name.append(c_mir1)\n",
    "                                mir_new_name.append(new_name)\n",
    "                                family_name.append(n_mir)\n",
    "                                novel_mir_seq.append(unknown_mir_dict[n_mir][-1])\n",
    "                                attribute.append('Paralogues')                                    \n",
    "                                if n_mir in paralogue_dict.keys():\n",
    "                                    paralogue_dict[n_mir].append(new_name)\n",
    "                                else:\n",
    "                                    paralogue_dict[n_mir] = []\n",
    "                                    paralogue_dict[n_mir].append(new_name)\n",
    "\n",
    "                                \n",
    "\n",
    "            \n",
    "            \n",
    "mir_reannotation['Old_miR_ID'] = mir_old_name\n",
    "mir_reannotation['New_miR_ID'] = mir_new_name\n",
    "mir_reannotation['Family'] = family_name\n",
    "mir_reannotation['Relation'] = attribute\n",
    "mir_reannotation['sequence'] = novel_mir_seq\n",
    "mir_reannotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict2 = {k:v for k,v in seed_dict.items() if 'hsa' in k}\n",
    "for idx in tqdm(range(novel_miRNA2.shape[0])):\n",
    "    novel_seq = novel_miRNA2.iloc[idx,3]\n",
    "    novel_seed = novel_seq[1:7]\n",
    "    suffix = 1\n",
    "    for k,v in seed_dict2.items():\n",
    "        if novel_seed == v:\n",
    "            if k+'_'+str(suffix) in novel_miRNA2.index:\n",
    "                paralog_index = novel_miRNA2[novel_miRNA2.index.str.contains(k)].index.tolist()\n",
    "                suffix = sorted([int(i.split('_')[-1]) for i in paralog_index])[-1]\n",
    "                new_name = k + '_' + str(suffix+1)\n",
    "            elif not '_' in k:\n",
    "                suffix = 1\n",
    "                new_name = k + '_' + str(suffix)\n",
    "            novel_miRNA2 = novel_miRNA2.rename(index={novel_miRNA2.index[idx] : new_name})\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = [counts_known2,novel_miRNA2]\n",
    "counts_final = pd.concat(frame)\n",
    "counts_final = counts_final.drop_duplicates(subset=['miRNA_sequence'], keep='first')\n",
    "counts_final.to_csv(\"data/counts_final.csv\",encoding='utf-8',index=True)\n",
    "print(counts_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_ground_truth = pd.read_csv('data/mirna2.csv')\n",
    "novelrna_ground_truth = pd.read_csv('data/novel_mirna2.csv')\n",
    "pirna_ground_truth = pd.read_csv('data/pirna2.csv')\n",
    "frame = [mirna_ground_truth,novelrna_ground_truth,pirna_ground_truth]\n",
    "ground_truth = pd.concat(frame)\n",
    "novelrna_ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rna_to_dna(seq):\n",
    "    for char in seq:\n",
    "        seq = list(seq)\n",
    "        char_idx = seq.index(char)\n",
    "        if char == 'u':\n",
    "            seq[char_idx] = 't'\n",
    "            \n",
    "    seq1 = \"\"\n",
    "    return seq1.join(seq).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirna_counts = pd.read_csv('data/piRNA/pirna_counts/synthetic_data_trimmed_pirna_counts.txt',sep='\\t')\n",
    "pirna_counts.columns = ['chr','database','type','chr_start','chr_end','Sequence','strand','extra','RNA_ID','Depth']\n",
    "pirna_counts =pirna_counts[pirna_counts['Depth']!=0]\n",
    "pirna_counts = pirna_counts[['RNA_ID','Sequence','Depth']]\n",
    "pirna_counts1 = pirna_counts[['RNA_ID','Depth']]\n",
    "pirna_counts1 = pirna_counts1.set_index('RNA_ID')\n",
    "pirna_counts = pirna_counts.set_index('RNA_ID')\n",
    "pirna_counts.to_csv('data/piRNA/pirna_counts/pirna_counts.csv',encoding='utf-8',index=True)\n",
    "# print(pirna_counts.shape)\n",
    "pirna_counts2 = pd.DataFrame(pirna_counts1.groupby(pirna_counts1.index)['Depth'].sum())\n",
    "# print(pirna_counts2.shape)\n",
    "\n",
    "mirna_count = pd.read_csv('data/counts_final.csv',index_col=0)\n",
    "mir_final1 = pd.DataFrame()\n",
    "mir_final1['RNA_ID'] = list(mirna_count.index)\n",
    "mir_final1['Sequence'] = [rna_to_dna(seq) for seq in mirna_count['miRNA_sequence']]\n",
    "mir_final1['Depth'] = list(mirna_count['synthetic_data_trimmed.result'])\n",
    "mir_final1 = mir_final1.set_index('RNA_ID')\n",
    "frame = [pirna_counts2,mir_final1]\n",
    "result = pd.concat(frame,sort=True)\n",
    "# result = pd.read_csv('data/counts_final.csv',index_col=0)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dict = {}\n",
    "for idx in tqdm(range(ground_truth.shape[0])):\n",
    "    seq = ground_truth.iloc[idx,4]\n",
    "    impurity = ground_truth.iloc[idx,1]\n",
    "    name = ground_truth.iloc[idx,0]\n",
    "    counts = int(ground_truth.iloc[idx,-1])\n",
    "    chr_loc = ground_truth.iloc[idx,5]\n",
    "    if seq in seq_dict.keys():\n",
    "        if seq_dict[seq][2] == impurity:\n",
    "            seq_dict[seq][-2] += counts\n",
    "    else:\n",
    "        seq_dict[seq] = [name,seq,impurity,counts,chr_loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mir_final2 = pd.DataFrame()\n",
    "actual_mir_name = []\n",
    "actual_count = []\n",
    "impurity = []\n",
    "# pir_list = [pir for pir in list(mir_ground_truth_final['RNA_ID']) if 'hsa-piR' in pir]\n",
    "seq_list = list(set(ground_truth['ref_Sequence']))\n",
    "for idx in tqdm(range(result.shape[0])):\n",
    "    seq = result.iloc[idx,1]\n",
    "    if str(seq) == 'nan':\n",
    "        for k,v in seq_dict.items():\n",
    "            if result.index[idx] == v[0] and v[-3] == 'None':\n",
    "#                 print(str(idx), k,v[0],pirna_counts2.loc[list(result.index)[idx],:])\n",
    "                mir_final2 = mir_final2.append(pirna_counts2.loc[list(result.index)[idx],:])\n",
    "                actual_mir_name.append(result.index[idx])\n",
    "                actual_count.append(v[-2])\n",
    "                impurity.append(v[-3])\n",
    "    else:\n",
    "        for seq1 in seq_list:\n",
    "            if seq1 in seq or seq in seq1:        \n",
    "                actual_mir_name.append(seq_dict[seq1][0])\n",
    "                actual_count.append(int(seq_dict[seq1][-2]))\n",
    "                impurity.append(seq_dict[seq1][-3])\n",
    "                if result.loc[[list(result.index)[idx]],:].shape[0] == 1:\n",
    "                    mir_final2 = mir_final2.append(result.loc[list(result.index)[idx],:])\n",
    "                else:\n",
    "                    df = result.loc[list(result.index)[idx],:]\n",
    "                    df2 = df.iloc[[0],:]                    \n",
    "                    df2['Depth'] = df['Depth'].sum()\n",
    "                    mir_final2 = mir_final2.append(df2.loc[df2.index[0],:])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "mir_final2['actual_name'] = actual_mir_name\n",
    "mir_final2['actual_counts'] = actual_count\n",
    "mir_final2['impurity'] = impurity\n",
    "mir_final2.to_csv(os.path.join('data/mirs_matched_with_ground_truth.csv'),encoding='utf-8',index=True)\n",
    "print(mir_final2.shape)\n",
    "mir_final2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_kmir_none = mirna_ground_truth[mirna_ground_truth['Impure_Region'].str.contains('None')]\n",
    "df_actual_kmir_seed = mirna_ground_truth[mirna_ground_truth['Impure_Region'].str.contains('^Seed_region')]\n",
    "df_actual_kmir_xseed = mirna_ground_truth[mirna_ground_truth['Impure_Region'].str.contains('Outside_Seed_region')]\n",
    "df_actual_kmir_both = mirna_ground_truth[mirna_ground_truth['Impure_Region'].str.contains('Both_region')]\n",
    "positive_counts_kmir = df_actual_kmir_none['Expression_count'].sum()\n",
    "negative_counts_kmir = df_actual_kmir_seed['Expression_count'].sum() + df_actual_kmir_xseed['Expression_count'].sum() + df_actual_kmir_both['Expression_count'].sum()\n",
    "\n",
    "df_actual_nmir_none = novelrna_ground_truth[novelrna_ground_truth['Impure_Region'].str.contains('None')]\n",
    "df_actual_nmir_seed = novelrna_ground_truth[novelrna_ground_truth['Impure_Region'].str.contains('^Seed_region')]\n",
    "df_actual_nmir_xseed = novelrna_ground_truth[novelrna_ground_truth['Impure_Region'].str.contains('Outside_Seed_region')]\n",
    "df_actual_nmir_both = novelrna_ground_truth[novelrna_ground_truth['Impure_Region'].str.contains('Both_region')]\n",
    "positive_counts_nmir = df_actual_nmir_none['Expression_count'].sum()\n",
    "negative_counts_nmir = df_actual_nmir_seed['Expression_count'].sum() + df_actual_nmir_xseed['Expression_count'].sum() + df_actual_nmir_both['Expression_count'].sum()\n",
    "\n",
    "df_actual_pmir_none = pirna_ground_truth[pirna_ground_truth['Impure_Region'].str.contains('None')]\n",
    "df_actual_pmir_seed = pirna_ground_truth[pirna_ground_truth['Impure_Region'].str.contains('^Seed_region')]\n",
    "df_actual_pmir_xseed = pirna_ground_truth[pirna_ground_truth['Impure_Region'].str.contains('Outside_Seed_region')]\n",
    "df_actual_pmir_both = pirna_ground_truth[pirna_ground_truth['Impure_Region'].str.contains('Both_region')]\n",
    "positive_counts_pmir = df_actual_pmir_none['Expression_count'].sum()\n",
    "negative_counts_pmir = df_actual_pmir_seed['Expression_count'].sum() + df_actual_pmir_xseed['Expression_count'].sum() + df_actual_pmir_both['Expression_count'].sum()\n",
    "\n",
    "total_positive = positive_counts_kmir + positive_counts_nmir + positive_counts_pmir\n",
    "total_negative = negative_counts_kmir + negative_counts_nmir + negative_counts_pmir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mir_final = pd.read_csv('data/mirs_matched_with_ground_truth.csv',index_col=0)\n",
    "df0 = mir_final[(mir_final['actual_name'].str.contains('hsa-miR')) |\\\n",
    "                (mir_final['actual_name'].str.contains('hsa-mir')) |\\\n",
    "               (mir_final['actual_name'].str.contains('hsa-let'))] \n",
    "df0 = df0.sort_values(by =\"Depth\", ascending=False)\n",
    "df0.drop_duplicates(subset =\"Sequence\", keep = 'first', inplace = True)\n",
    "for idx in range(df0.shape[0]):\n",
    "    if df0.iloc[idx,0] > df0.iloc[idx,3]:\n",
    "        df0.iloc[idx,0] = df0.iloc[idx,3]\n",
    "\n",
    "df = df0[(df0.index.str.contains('hsa-mir')) |\\\n",
    "         (df0.index.str.contains('hsa-miR')) |\\\n",
    "        (df0.index.str.contains('hsa-let'))]\n",
    "df1_k = df[df['impurity']=='None']\n",
    "df1_1k = df[df['impurity']!='None']\n",
    "df1 = df0[df0.index.str.contains('novel')]\n",
    "df2 = df0[df0.index.str.contains('hsa-piR')]    \n",
    "k_k = df1_k['Depth'].sum()\n",
    "if k_k > positive_counts_kmir:\n",
    "    print('k_k is equal to positive_counts_kmir')\n",
    "    k_k = positive_counts_kmir\n",
    "\n",
    "k_n = df1['Depth'].sum()\n",
    "k_p = df2['Depth'].sum()\n",
    "k_o = positive_counts_kmir - k_k - k_n - k_p\n",
    "c_matrix = pd.DataFrame(index = [\"known_mirna\",\"novel_mirna\",\"pirna\",\"other\"], columns=[\"known_mirna\",\"novel_mirna\",\"pirna\",\"other\"])\n",
    "c_matrix.iloc[0,0] = k_k\n",
    "c_matrix.iloc[1,0] = k_n\n",
    "c_matrix.iloc[2,0] = k_p\n",
    "c_matrix.iloc[3,0] = k_o\n",
    "\n",
    "# For novel miRNA    \n",
    "df0 = mir_final[((mir_final['actual_name'].str.contains('novel')))]\n",
    "df0 = df0.sort_values(by=['Depth'], ascending=False)\n",
    "df0.drop_duplicates(subset =\"Sequence\", keep = 'first', inplace = True)\n",
    "for idx in range(df0.shape[0]):\n",
    "    if df0.iloc[idx,0] > df0.iloc[idx,3]:\n",
    "        df0.iloc[idx,0] = df0.iloc[idx,3]\n",
    "df1 = df0[df0.index.str.contains('novel')]\n",
    "df1_n = df1[df1['impurity']=='None']\n",
    "df1_1n = df1[df1['impurity']!='None']\n",
    "df2 = df0[df0.index.str.contains('hsa-mir') | df0.index.str.contains('hsa-miR')]\n",
    "df3 = df0[df0.index.str.contains('hsa-piR')]        \n",
    "n_k = df2['Depth'].sum()\n",
    "n_n = df1_n['Depth'].sum()\n",
    "if n_n > positive_counts_nmir:\n",
    "    print('n_n is equal to positive_counts_nmir')\n",
    "    n_n = positive_counts_nmir\n",
    "\n",
    "n_p = df3['Depth'].sum()\n",
    "n_o = positive_counts_nmir - n_k - n_n - n_p\n",
    "c_matrix.iloc[0,1] = n_k\n",
    "c_matrix.iloc[1,1] = n_n\n",
    "c_matrix.iloc[2,1] = n_p\n",
    "c_matrix.iloc[3,1] = n_o\n",
    "\n",
    "# For piRNA\n",
    "df0 = mir_final[((mir_final['actual_name'].str.contains('hsa-piR')))]\n",
    "for idx in range(df0.shape[0]):\n",
    "    if df0.iloc[idx,0] > df0.iloc[idx,3]:\n",
    "        df0.iloc[idx,0] = df0.iloc[idx,3]\n",
    "df1 = df0[df0.index.str.contains('hsa-mir') | df0.index.str.contains('hsa-miR')]\n",
    "df2 = df0[df0.index.str.contains('novel')]\n",
    "df3 = df0[df0.index.str.contains('piR')]\n",
    "df1_p = df3[df3['impurity']=='None']\n",
    "df1_1p = df3[df3['impurity']!='None']\n",
    "p_k = df1['Depth'].sum()\n",
    "p_n = df2['Depth'].sum()\n",
    "p_p = df1_p['Depth'].sum()\n",
    "p_o = positive_counts_pmir - p_k - p_n - p_p\n",
    "c_matrix.iloc[0,2] = p_k\n",
    "c_matrix.iloc[1,2] = p_n\n",
    "c_matrix.iloc[2,2] = p_p\n",
    "c_matrix.iloc[3,2] = p_o\n",
    "\n",
    "# For remaining reads\n",
    "o_k = df1_1k['Depth'].sum()\n",
    "o_n = df1_1n['Depth'].sum()\n",
    "o_p = df1_1p['Depth'].sum()\n",
    "o_o = total_negative - o_k - o_n - o_p\n",
    "\n",
    "c_matrix.iloc[0,3] = o_k\n",
    "c_matrix.iloc[1,3] = o_n\n",
    "c_matrix.iloc[2,3] = o_p\n",
    "c_matrix.iloc[3,3] = o_o\n",
    "\n",
    "c_matrix.to_csv(\"data/multi_class_c_matrix.csv\",encoding=\"utf-8\",index=True)\n",
    "\n",
    "cm = c_matrix.to_numpy()\n",
    "c = c_matrix\n",
    "# for known miRNA\n",
    "tp_k = c.iloc[0,0]\n",
    "fn_k = c['known_mirna'][1:].sum()\n",
    "fp_k = c.iloc[0,1:].sum()\n",
    "tn_k = np.trace(c) - tp_k\n",
    "acc = 100*(tp_k+tn_k)/(tp_k+tn_k+fp_k+fn_k)\n",
    "prec = 100*tp_k/(tp_k+fp_k)\n",
    "sen = 100*tp_k/(tp_k+fn_k)\n",
    "spec = 100*tn_k/(fp_k+tn_k)\n",
    "f1_sc = 100*2*tp_k/(2*tp_k+fp_k+fn_k)\n",
    "k_mir_param = [acc,prec,sen,spec,f1_sc]\n",
    "k_mir_param = list(map(float, k_mir_param))\n",
    "\n",
    "# for novel miRNA\n",
    "tp_n = c.iloc[1,1]\n",
    "\n",
    "fn_n = c.iloc[0,1] + c['novel_mirna'][2:].sum()\n",
    "fp_n = c.iloc[1,0] + c.iloc[1,2:].sum()\n",
    "tn_n = np.trace(c) - tp_n\n",
    "if tp_n == 0:\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    sen = 0\n",
    "    spec = 0\n",
    "    f1_sc = 0\n",
    "else:\n",
    "    acc = 100*(tp_n+tn_n)/(tp_n+tn_n+fp_n+fn_n)\n",
    "    prec = 100*tp_n/(tp_n+fp_n)\n",
    "    sen = 100*tp_n/(tp_n+fn_n)\n",
    "    spec = 100*tn_n/(fp_n+tn_n)\n",
    "    f1_sc = 100*2*tp_n/(2*tp_n+fp_n+fn_n)\n",
    "n_mir_param = [acc,prec,sen,spec,f1_sc]\n",
    "n_mir_param = list(map(float, n_mir_param))\n",
    "\n",
    "# for piRNA\n",
    "tp_p = c.iloc[2,2]\n",
    "fn_p = c['pirna'][:2].sum() + c.iloc[3,2]\n",
    "fp_p = c.iloc[2,3] + c.iloc[2,:2].sum()\n",
    "tn_p = np.trace(c) - tp_p\n",
    "if tp_p == 0:\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    sen = 0\n",
    "    spec = 0\n",
    "    f1_sc = 0\n",
    "else:\n",
    "    acc = 100*(tp_p+tn_p)/(tp_p+tn_p+fp_p+fn_p)\n",
    "    prec = 100*tp_p/(tp_p+fp_p)\n",
    "    sen = 100*tp_p/(tp_p+fn_p)\n",
    "    spec = 100*tn_p/(fp_p+tn_p)\n",
    "    f1_sc = 100*2*tp_p/(2*tp_p+fp_p+fn_p)\n",
    "p_mir_param = [acc,prec,sen,spec,f1_sc]\n",
    "p_mir_param = list(map(float, p_mir_param))\n",
    "\n",
    "# for all RNA\n",
    "acc = 100*np.trace(c)/c.to_numpy().sum()\n",
    "prec = np.mean([k_mir_param[1],n_mir_param[1],p_mir_param[1]])\n",
    "sen = np.mean([k_mir_param[2],n_mir_param[2],p_mir_param[2]])\n",
    "spec = np.mean([k_mir_param[3],n_mir_param[3],p_mir_param[3]])\n",
    "f1_sc = np.mean([k_mir_param[4],n_mir_param[4],p_mir_param[4]])\n",
    "all_mir_param = [acc,prec,sen,spec,f1_sc]\n",
    "\n",
    "out_f = pd.DataFrame()\n",
    "out_f['param'] = ['accuracy', 'precision', 'sensitivity', 'specificity', 'f1_score']\n",
    "out_f['known_mirna'] = k_mir_param\n",
    "out_f['novel_mirna'] = n_mir_param\n",
    "out_f['pirna'] = p_mir_param\n",
    "out_f['overall'] = all_mir_param\n",
    "out_f = out_f.set_index('param')\n",
    "out_f.to_csv(\"data/multi_classc_matrix_performance.csv\",encoding='utf-8',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/multi_class_c_matrix.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/multi_classc_matrix_performance.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two output files will be generated after completing the synthetic data experiment using this notebook. These files are as follows:\n",
    "\n",
    "    1. multi_class_c_matrix.csv: This file contains all the sequence counts for each category of RNA.\n",
    "    \n",
    "    2. multi_classc_matrix_performance.csv: This file contains all the performance matrices for miRPipe pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Open source RNA-Seq pipeline for identification of novel-mirs and their gene regulatory networks",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "181.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
